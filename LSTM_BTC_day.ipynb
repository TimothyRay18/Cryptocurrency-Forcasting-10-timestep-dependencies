{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499ef9d1-2655-483e-9f50-0badccf17d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for lstm\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b213829a-295a-46a7-bbd7-152bcce0aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set-up to have matplotlib use its support for notebook inline plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ed0706-d5c2-4868-beee-6933c00b0a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unix</th>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume BTC</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>tradecount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.669766e+12</td>\n",
       "      <td>2022-11-30 00:00:00</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>16442.91</td>\n",
       "      <td>16466.66</td>\n",
       "      <td>16428.30</td>\n",
       "      <td>16443.60</td>\n",
       "      <td>2323.64958</td>\n",
       "      <td>3.822050e+07</td>\n",
       "      <td>48232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.669680e+12</td>\n",
       "      <td>2022-11-29 00:00:00</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>16212.18</td>\n",
       "      <td>16548.71</td>\n",
       "      <td>16100.00</td>\n",
       "      <td>16442.53</td>\n",
       "      <td>248106.25009</td>\n",
       "      <td>4.072651e+09</td>\n",
       "      <td>5206638.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.669594e+12</td>\n",
       "      <td>2022-11-28 00:00:00</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>16428.77</td>\n",
       "      <td>16487.04</td>\n",
       "      <td>15995.27</td>\n",
       "      <td>16212.91</td>\n",
       "      <td>252695.40367</td>\n",
       "      <td>4.096399e+09</td>\n",
       "      <td>5570407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.669507e+12</td>\n",
       "      <td>2022-11-27 00:00:00</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>16457.61</td>\n",
       "      <td>16600.00</td>\n",
       "      <td>16401.00</td>\n",
       "      <td>16428.78</td>\n",
       "      <td>162025.47607</td>\n",
       "      <td>2.679049e+09</td>\n",
       "      <td>3362221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.669421e+12</td>\n",
       "      <td>2022-11-26 00:00:00</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>16521.35</td>\n",
       "      <td>16701.99</td>\n",
       "      <td>16385.00</td>\n",
       "      <td>16458.57</td>\n",
       "      <td>181804.81666</td>\n",
       "      <td>3.011990e+09</td>\n",
       "      <td>3817130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>1.503274e+09</td>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>4086.29</td>\n",
       "      <td>4119.62</td>\n",
       "      <td>3911.79</td>\n",
       "      <td>4016.00</td>\n",
       "      <td>685.12000</td>\n",
       "      <td>2.770592e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>1.503187e+09</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>4211.08</td>\n",
       "      <td>4032.62</td>\n",
       "      <td>4086.29</td>\n",
       "      <td>463.54000</td>\n",
       "      <td>1.915636e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>1.503101e+09</td>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>371.15000</td>\n",
       "      <td>1.508239e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>1.503014e+09</td>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>1178.07000</td>\n",
       "      <td>4.994494e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>1.502928e+09</td>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>4469.93</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>647.86000</td>\n",
       "      <td>2.812379e+06</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1933 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              unix                 date    symbol      open      high  \\\n",
       "0     1.669766e+12  2022-11-30 00:00:00  BTC/USDT  16442.91  16466.66   \n",
       "1     1.669680e+12  2022-11-29 00:00:00  BTC/USDT  16212.18  16548.71   \n",
       "2     1.669594e+12  2022-11-28 00:00:00  BTC/USDT  16428.77  16487.04   \n",
       "3     1.669507e+12  2022-11-27 00:00:00  BTC/USDT  16457.61  16600.00   \n",
       "4     1.669421e+12  2022-11-26 00:00:00  BTC/USDT  16521.35  16701.99   \n",
       "...            ...                  ...       ...       ...       ...   \n",
       "1928  1.503274e+09           2017-08-21  BTC/USDT   4086.29   4119.62   \n",
       "1929  1.503187e+09           2017-08-20  BTC/USDT   4139.98   4211.08   \n",
       "1930  1.503101e+09           2017-08-19  BTC/USDT   4108.37   4184.69   \n",
       "1931  1.503014e+09           2017-08-18  BTC/USDT   4285.08   4371.52   \n",
       "1932  1.502928e+09           2017-08-17  BTC/USDT   4469.93   4485.39   \n",
       "\n",
       "           low     close    Volume BTC   Volume USDT  tradecount  \n",
       "0     16428.30  16443.60    2323.64958  3.822050e+07     48232.0  \n",
       "1     16100.00  16442.53  248106.25009  4.072651e+09   5206638.0  \n",
       "2     15995.27  16212.91  252695.40367  4.096399e+09   5570407.0  \n",
       "3     16401.00  16428.78  162025.47607  2.679049e+09   3362221.0  \n",
       "4     16385.00  16458.57  181804.81666  3.011990e+09   3817130.0  \n",
       "...        ...       ...           ...           ...         ...  \n",
       "1928   3911.79   4016.00     685.12000  2.770592e+06         NaN  \n",
       "1929   4032.62   4086.29     463.54000  1.915636e+06         NaN  \n",
       "1930   3850.00   4139.98     371.15000  1.508239e+06         NaN  \n",
       "1931   3938.77   4108.37    1178.07000  4.994494e+06         NaN  \n",
       "1932   4200.74   4285.08     647.86000  2.812379e+06         NaN  \n",
       "\n",
       "[1933 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_dfd = pd.read_csv('Binance_BTCUSDT_d.csv')\n",
    "btc_dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41062755-9cca-4348-91bc-e80d46b79c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume USDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>16442.91</td>\n",
       "      <td>16466.66</td>\n",
       "      <td>16428.30</td>\n",
       "      <td>16443.60</td>\n",
       "      <td>3.822050e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>16212.18</td>\n",
       "      <td>16548.71</td>\n",
       "      <td>16100.00</td>\n",
       "      <td>16442.53</td>\n",
       "      <td>4.072651e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>16428.77</td>\n",
       "      <td>16487.04</td>\n",
       "      <td>15995.27</td>\n",
       "      <td>16212.91</td>\n",
       "      <td>4.096399e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>16457.61</td>\n",
       "      <td>16600.00</td>\n",
       "      <td>16401.00</td>\n",
       "      <td>16428.78</td>\n",
       "      <td>2.679049e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>16521.35</td>\n",
       "      <td>16701.99</td>\n",
       "      <td>16385.00</td>\n",
       "      <td>16458.57</td>\n",
       "      <td>3.011990e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>15059.56</td>\n",
       "      <td>17176.24</td>\n",
       "      <td>14600.00</td>\n",
       "      <td>16960.39</td>\n",
       "      <td>3.693220e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>15280.00</td>\n",
       "      <td>13918.04</td>\n",
       "      <td>15059.54</td>\n",
       "      <td>3.127816e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>14690.00</td>\n",
       "      <td>15307.56</td>\n",
       "      <td>14150.00</td>\n",
       "      <td>14919.51</td>\n",
       "      <td>2.361169e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>13382.16</td>\n",
       "      <td>15473.49</td>\n",
       "      <td>12890.02</td>\n",
       "      <td>14675.11</td>\n",
       "      <td>2.797171e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>13715.65</td>\n",
       "      <td>13818.55</td>\n",
       "      <td>12750.00</td>\n",
       "      <td>13380.00</td>\n",
       "      <td>1.147997e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1795 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      open      high       low     close   Volume USDT\n",
       "0     2022-11-30  16442.91  16466.66  16428.30  16443.60  3.822050e+07\n",
       "1     2022-11-29  16212.18  16548.71  16100.00  16442.53  4.072651e+09\n",
       "2     2022-11-28  16428.77  16487.04  15995.27  16212.91  4.096399e+09\n",
       "3     2022-11-27  16457.61  16600.00  16401.00  16428.78  2.679049e+09\n",
       "4     2022-11-26  16521.35  16701.99  16385.00  16458.57  3.011990e+09\n",
       "...          ...       ...       ...       ...       ...           ...\n",
       "1790  2018-01-05  15059.56  17176.24  14600.00  16960.39  3.693220e+08\n",
       "1791  2018-01-04  14919.51  15280.00  13918.04  15059.54  3.127816e+08\n",
       "1792  2018-01-03  14690.00  15307.56  14150.00  14919.51  2.361169e+08\n",
       "1793  2018-01-02  13382.16  15473.49  12890.02  14675.11  2.797171e+08\n",
       "1794  2018-01-01  13715.65  13818.55  12750.00  13380.00  1.147997e+08\n",
       "\n",
       "[1795 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_dfd['date'] = pd.to_datetime(btc_dfd['date']).dt.date\n",
    "btc_clean = btc_dfd.drop(['unix', 'symbol', 'Volume BTC', 'tradecount'], axis=1)\n",
    "res_btc = btc_clean[~(btc_clean['date'] < pd.to_datetime(\"2018-01-01\").date())]\n",
    "res_btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc368c1b-a82f-4dbf-85c4-f19ebff7a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>Volume USDT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>2017-08-17</td>\n",
       "      <td>4469.93</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>2.812379e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>2017-08-18</td>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4.994494e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>2017-08-19</td>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>1.508239e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>4139.98</td>\n",
       "      <td>4211.08</td>\n",
       "      <td>4032.62</td>\n",
       "      <td>4086.29</td>\n",
       "      <td>1.915636e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>2017-08-21</td>\n",
       "      <td>4086.29</td>\n",
       "      <td>4119.62</td>\n",
       "      <td>3911.79</td>\n",
       "      <td>4016.00</td>\n",
       "      <td>2.770592e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-26</td>\n",
       "      <td>16521.35</td>\n",
       "      <td>16701.99</td>\n",
       "      <td>16385.00</td>\n",
       "      <td>16458.57</td>\n",
       "      <td>3.011990e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-27</td>\n",
       "      <td>16457.61</td>\n",
       "      <td>16600.00</td>\n",
       "      <td>16401.00</td>\n",
       "      <td>16428.78</td>\n",
       "      <td>2.679049e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-28</td>\n",
       "      <td>16428.77</td>\n",
       "      <td>16487.04</td>\n",
       "      <td>15995.27</td>\n",
       "      <td>16212.91</td>\n",
       "      <td>4.096399e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-29</td>\n",
       "      <td>16212.18</td>\n",
       "      <td>16548.71</td>\n",
       "      <td>16100.00</td>\n",
       "      <td>16442.53</td>\n",
       "      <td>4.072651e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>16442.91</td>\n",
       "      <td>16466.66</td>\n",
       "      <td>16428.30</td>\n",
       "      <td>16443.60</td>\n",
       "      <td>3.822050e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1933 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date      open      high       low     close   Volume USDT\n",
       "1932  2017-08-17   4469.93   4485.39   4200.74   4285.08  2.812379e+06\n",
       "1931  2017-08-18   4285.08   4371.52   3938.77   4108.37  4.994494e+06\n",
       "1930  2017-08-19   4108.37   4184.69   3850.00   4139.98  1.508239e+06\n",
       "1929  2017-08-20   4139.98   4211.08   4032.62   4086.29  1.915636e+06\n",
       "1928  2017-08-21   4086.29   4119.62   3911.79   4016.00  2.770592e+06\n",
       "...          ...       ...       ...       ...       ...           ...\n",
       "4     2022-11-26  16521.35  16701.99  16385.00  16458.57  3.011990e+09\n",
       "3     2022-11-27  16457.61  16600.00  16401.00  16428.78  2.679049e+09\n",
       "2     2022-11-28  16428.77  16487.04  15995.27  16212.91  4.096399e+09\n",
       "1     2022-11-29  16212.18  16548.71  16100.00  16442.53  4.072651e+09\n",
       "0     2022-11-30  16442.91  16466.66  16428.30  16443.60  3.822050e+07\n",
       "\n",
       "[1933 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc = btc_clean.iloc[::-1]\n",
    "btc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483f3731-9efb-47c6-af9c-6eadc95287e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc = btc[['open', 'high', 'low', 'Volume USDT', 'close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c538c834-3cdb-4949-9599-187261b0a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4469.93</td>\n",
       "      <td>4485.39</td>\n",
       "      <td>4200.74</td>\n",
       "      <td>2.812379e+06</td>\n",
       "      <td>4285.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4285.08</td>\n",
       "      <td>4371.52</td>\n",
       "      <td>3938.77</td>\n",
       "      <td>4.994494e+06</td>\n",
       "      <td>4108.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4108.37</td>\n",
       "      <td>4184.69</td>\n",
       "      <td>3850.00</td>\n",
       "      <td>1.508239e+06</td>\n",
       "      <td>4139.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4139.98</td>\n",
       "      <td>4211.08</td>\n",
       "      <td>4032.62</td>\n",
       "      <td>1.915636e+06</td>\n",
       "      <td>4086.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4086.29</td>\n",
       "      <td>4119.62</td>\n",
       "      <td>3911.79</td>\n",
       "      <td>2.770592e+06</td>\n",
       "      <td>4016.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>16521.35</td>\n",
       "      <td>16701.99</td>\n",
       "      <td>16385.00</td>\n",
       "      <td>3.011990e+09</td>\n",
       "      <td>16458.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>16457.61</td>\n",
       "      <td>16600.00</td>\n",
       "      <td>16401.00</td>\n",
       "      <td>2.679049e+09</td>\n",
       "      <td>16428.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>16428.77</td>\n",
       "      <td>16487.04</td>\n",
       "      <td>15995.27</td>\n",
       "      <td>4.096399e+09</td>\n",
       "      <td>16212.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>16212.18</td>\n",
       "      <td>16548.71</td>\n",
       "      <td>16100.00</td>\n",
       "      <td>4.072651e+09</td>\n",
       "      <td>16442.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>16442.91</td>\n",
       "      <td>16466.66</td>\n",
       "      <td>16428.30</td>\n",
       "      <td>3.822050e+07</td>\n",
       "      <td>16443.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1933 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open      high       low   Volume USDT     close\n",
       "0      4469.93   4485.39   4200.74  2.812379e+06   4285.08\n",
       "1      4285.08   4371.52   3938.77  4.994494e+06   4108.37\n",
       "2      4108.37   4184.69   3850.00  1.508239e+06   4139.98\n",
       "3      4139.98   4211.08   4032.62  1.915636e+06   4086.29\n",
       "4      4086.29   4119.62   3911.79  2.770592e+06   4016.00\n",
       "...        ...       ...       ...           ...       ...\n",
       "1928  16521.35  16701.99  16385.00  3.011990e+09  16458.57\n",
       "1929  16457.61  16600.00  16401.00  2.679049e+09  16428.78\n",
       "1930  16428.77  16487.04  15995.27  4.096399e+09  16212.91\n",
       "1931  16212.18  16548.71  16100.00  4.072651e+09  16442.53\n",
       "1932  16442.91  16466.66  16428.30  3.822050e+07  16443.60\n",
       "\n",
       "[1933 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = btc.reset_index(drop=True)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140ecfee-f8cf-45f6-b3c6-49e8e7458101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c42a7847-70b8-4487-a874-67e370666724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [open, high, low, Volume USDT, close]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = btc[btc.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8e525a6-83d0-4c8b-9875-33e0e15480ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08213717-a303-4c9e-97bb-280204035109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "open           0\n",
       "high           0\n",
       "low            0\n",
       "Volume USDT    0\n",
       "close          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff76bd7-16f2-4bf2-8a64-f1f6a1194661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51dc56af-5903-45ab-a70b-556fb3f3d97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "close_df = scaler.fit_transform(np.array(model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d98152d5-e8c5-4645-90ea-876a042b25e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99094453e-02, 1.83935731e-02, 2.18236933e-02, 1.24996525e-04,\n",
       "        1.70362814e-02],\n",
       "       [1.70362841e-02, 1.66610117e-02, 1.76920262e-02, 2.72862138e-04,\n",
       "        1.42896423e-02],\n",
       "       [1.42896445e-02, 1.38183450e-02, 1.62919877e-02, 3.66247078e-05,\n",
       "        1.47809629e-02],\n",
       "       ...,\n",
       "       [2.05788134e-01, 2.01001773e-01, 2.07841446e-01, 2.77516787e-01,\n",
       "        2.02432946e-01],\n",
       "       [2.02421631e-01, 2.01940098e-01, 2.09493198e-01, 2.75907595e-01,\n",
       "        2.06001976e-01],\n",
       "       [2.06007915e-01, 2.00691686e-01, 2.14670990e-01, 2.52434099e-03,\n",
       "        2.06018607e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d04ca0-f51f-465b-8229-af015c624039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:  (1739, 5)\n",
      "test_data:  (194, 5)\n"
     ]
    }
   ],
   "source": [
    "training_size = int(len(close_df)*0.9)\n",
    "test_size = len(close_df)-training_size\n",
    "train_data,test_data = close_df[0:training_size,:],close_df[training_size:len(close_df),:]\n",
    "print('train_data: ', train_data.shape)\n",
    "print('test_data: ', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d06c56c-8ef3-4e92-8c96-99b93dbaaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step)]  \n",
    "        b = dataset[i+time_step][4]\n",
    "        # print(a)\n",
    "        # print(type(b))\n",
    "        dataX.append(a)\n",
    "        dataY.append(float(b))\n",
    "    \n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894700da-fd83-4d14-bedd-39f52b99890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (1728, 10, 5)\n",
      "X_test:  (183, 10, 5)\n"
     ]
    }
   ],
   "source": [
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "# reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "# X_train = X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "# X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4b787e-f47b-459d-97fa-887ef0780417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.99094453e-02, 1.83935731e-02, 2.18236933e-02, 1.24996525e-04,\n",
       "         1.70362814e-02],\n",
       "        [1.70362841e-02, 1.66610117e-02, 1.76920262e-02, 2.72862138e-04,\n",
       "         1.42896423e-02],\n",
       "        [1.42896445e-02, 1.38183450e-02, 1.62919877e-02, 3.66247078e-05,\n",
       "         1.47809629e-02],\n",
       "        ...,\n",
       "        [1.43773082e-02, 1.66634461e-02, 1.99984544e-02, 1.52984262e-04,\n",
       "         1.75170326e-02],\n",
       "        [1.75170354e-02, 1.79145968e-02, 2.25608544e-02, 9.97851124e-05,\n",
       "         1.69678913e-02],\n",
       "        [1.69678940e-02, 1.65922387e-02, 2.20077470e-02, 0.00000000e+00,\n",
       "         1.78501234e-02]],\n",
       "\n",
       "       [[1.70362841e-02, 1.66610117e-02, 1.76920262e-02, 2.72862138e-04,\n",
       "         1.42896423e-02],\n",
       "        [1.42896445e-02, 1.38183450e-02, 1.62919877e-02, 3.66247078e-05,\n",
       "         1.47809629e-02],\n",
       "        [1.47809652e-02, 1.42198757e-02, 1.91721841e-02, 6.42309598e-05,\n",
       "         1.39464484e-02],\n",
       "        ...,\n",
       "        [1.75170354e-02, 1.79145968e-02, 2.25608544e-02, 9.97851124e-05,\n",
       "         1.69678913e-02],\n",
       "        [1.69678940e-02, 1.65922387e-02, 2.20077470e-02, 0.00000000e+00,\n",
       "         1.78501234e-02],\n",
       "        [1.78501262e-02, 1.70943422e-02, 2.31611188e-02, 3.65807008e-05,\n",
       "         1.74237734e-02]],\n",
       "\n",
       "       [[1.42896445e-02, 1.38183450e-02, 1.62919877e-02, 3.66247078e-05,\n",
       "         1.47809629e-02],\n",
       "        [1.47809652e-02, 1.42198757e-02, 1.91721841e-02, 6.42309598e-05,\n",
       "         1.39464484e-02],\n",
       "        [1.39464506e-02, 1.28282882e-02, 1.72665104e-02, 1.22164929e-04,\n",
       "         1.28539168e-02],\n",
       "        ...,\n",
       "        [1.69678940e-02, 1.65922387e-02, 2.20077470e-02, 0.00000000e+00,\n",
       "         1.78501234e-02],\n",
       "        [1.78501262e-02, 1.70943422e-02, 2.31611188e-02, 3.65807008e-05,\n",
       "         1.74237734e-02],\n",
       "        [1.74237761e-02, 1.70916035e-02, 2.06219029e-02, 1.06987929e-04,\n",
       "         1.86156261e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[5.10201782e-01, 5.00122483e-01, 5.04184186e-01, 8.27767960e-02,\n",
       "         5.01786924e-01],\n",
       "        [5.01787158e-01, 4.90505223e-01, 4.87292092e-01, 1.69229987e-01,\n",
       "         4.79498129e-01],\n",
       "        [4.79498048e-01, 4.71165565e-01, 4.29243093e-01, 4.17783108e-01,\n",
       "         4.17914566e-01],\n",
       "        ...,\n",
       "        [4.05647001e-01, 4.11827885e-01, 4.07110435e-01, 1.01678752e-01,\n",
       "         4.18076681e-01],\n",
       "        [4.18076746e-01, 4.28819220e-01, 4.20516234e-01, 9.49036146e-02,\n",
       "         4.37383669e-01],\n",
       "        [4.37383737e-01, 4.26824500e-01, 4.14318654e-01, 1.48034039e-01,\n",
       "         4.14770176e-01]],\n",
       "\n",
       "       [[5.01787158e-01, 4.90505223e-01, 4.87292092e-01, 1.69229987e-01,\n",
       "         4.79498129e-01],\n",
       "        [4.79498048e-01, 4.71165565e-01, 4.29243093e-01, 4.17783108e-01,\n",
       "         4.17914566e-01],\n",
       "        [4.17882301e-01, 4.47062162e-01, 4.24465424e-01, 3.50934675e-01,\n",
       "         4.32537454e-01],\n",
       "        ...,\n",
       "        [4.18076746e-01, 4.28819220e-01, 4.20516234e-01, 9.49036146e-02,\n",
       "         4.37383669e-01],\n",
       "        [4.37383737e-01, 4.26824500e-01, 4.14318654e-01, 1.48034039e-01,\n",
       "         4.14770176e-01],\n",
       "        [4.14770240e-01, 4.18600196e-01, 4.20049081e-01, 1.16154570e-01,\n",
       "         4.23644100e-01]],\n",
       "\n",
       "       [[4.79498048e-01, 4.71165565e-01, 4.29243093e-01, 4.17783108e-01,\n",
       "         4.17914566e-01],\n",
       "        [4.17882301e-01, 4.47062162e-01, 4.24465424e-01, 3.50934675e-01,\n",
       "         4.32537454e-01],\n",
       "        [4.32537677e-01, 4.39509308e-01, 3.93783495e-01, 4.26858469e-01,\n",
       "         4.02800823e-01],\n",
       "        ...,\n",
       "        [4.37383737e-01, 4.26824500e-01, 4.14318654e-01, 1.48034039e-01,\n",
       "         4.14770176e-01],\n",
       "        [4.14770240e-01, 4.18600196e-01, 4.20049081e-01, 1.16154570e-01,\n",
       "         4.23644100e-01],\n",
       "        [4.23644166e-01, 4.17407624e-01, 4.07496365e-01, 1.19389195e-01,\n",
       "         3.96760424e-01]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd6b140-b080-4165-bab5-3736ec95b4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01742377, 0.01861563, 0.02173655, ..., 0.4236441 , 0.39676042,\n",
       "       0.42169032])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51f6e253-7b00-4bb2-95dd-4a4a70061cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train\n",
    "train_y = y_train\n",
    "test_X = X_test\n",
    "test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "869ef0ff-56bc-45e3-aed4-a88338222b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1728, 10, 5) (1728,) (183, 10, 5) (183,)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43011a-1b18-4fef-b638-d84109a5347b",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6e3c5f-f40b-427b-bbf6-715de9fbfed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 25, 50),\n",
       " (16, 25, 60),\n",
       " (16, 25, 100),\n",
       " (16, 50, 50),\n",
       " (16, 50, 60),\n",
       " (16, 50, 100),\n",
       " (16, 100, 50),\n",
       " (16, 100, 60),\n",
       " (16, 100, 100),\n",
       " (32, 25, 50),\n",
       " (32, 25, 60),\n",
       " (32, 25, 100),\n",
       " (32, 50, 50),\n",
       " (32, 50, 60),\n",
       " (32, 50, 100),\n",
       " (32, 100, 50),\n",
       " (32, 100, 60),\n",
       " (32, 100, 100),\n",
       " (64, 25, 50),\n",
       " (64, 25, 60),\n",
       " (64, 25, 100),\n",
       " (64, 50, 50),\n",
       " (64, 50, 60),\n",
       " (64, 50, 100),\n",
       " (64, 100, 50),\n",
       " (64, 100, 60),\n",
       " (64, 100, 100)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = []\n",
    "batch = [16, 32, 64]\n",
    "epoch = [25, 50, 100]\n",
    "neuron = [50, 60, 100]\n",
    "for j in batch:\n",
    "    for k in epoch:\n",
    "        for l in neuron:\n",
    "            hyperparams.append((j,k,l))\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c52fc162-d986-4cb9-9893-7619681dfc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 25, 50),\n",
       " (16, 25, 60),\n",
       " (16, 25, 100),\n",
       " (16, 50, 50),\n",
       " (16, 50, 60),\n",
       " (16, 50, 100),\n",
       " (16, 100, 50),\n",
       " (16, 100, 60),\n",
       " (16, 100, 100)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam1 = hyperparams[:9]\n",
    "hyperparam2 = hyperparams[9:18]\n",
    "hyperparam3 = hyperparams[18:27]\n",
    "hyperparam1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d42226bd-8cc7-4f34-bb16-195c46f1e33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(32, 25, 50),\n",
       " (32, 25, 60),\n",
       " (32, 25, 100),\n",
       " (32, 50, 50),\n",
       " (32, 50, 60),\n",
       " (32, 50, 100),\n",
       " (32, 100, 50),\n",
       " (32, 100, 60),\n",
       " (32, 100, 100)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed77be16-ab42-4b03-b115-2019aff75427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 25, 50),\n",
       " (64, 25, 60),\n",
       " (64, 25, 100),\n",
       " (64, 50, 50),\n",
       " (64, 50, 60),\n",
       " (64, 50, 100),\n",
       " (64, 100, 50),\n",
       " (64, 100, 60),\n",
       " (64, 100, 100)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparam3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ffd8dc1-5eb8-4d7e-aba2-cab78df020ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:11:42.204865: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-15 21:11:42.206601: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:11:42.597674: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-15 21:11:42.989288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:11:43.069942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:11:43.958111: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:11:45.132922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:11:45.165772: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 3s - loss: 0.0060 - val_loss: 0.0026 - 3s/epoch - 25ms/step\n",
      "Epoch 2/25\n",
      "108/108 - 1s - loss: 0.0050 - val_loss: 0.0090 - 720ms/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "108/108 - 1s - loss: 0.0025 - val_loss: 0.0042 - 714ms/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "108/108 - 1s - loss: 0.0014 - val_loss: 0.0030 - 728ms/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0022 - 743ms/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0018 - 736ms/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0016 - 895ms/epoch - 8ms/step\n",
      "Epoch 8/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0015 - 734ms/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "108/108 - 1s - loss: 0.0010 - val_loss: 0.0014 - 813ms/epoch - 8ms/step\n",
      "Epoch 10/25\n",
      "108/108 - 1s - loss: 0.0010 - val_loss: 0.0014 - 738ms/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "108/108 - 1s - loss: 9.7347e-04 - val_loss: 0.0013 - 722ms/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "108/108 - 1s - loss: 9.2948e-04 - val_loss: 0.0013 - 742ms/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "108/108 - 1s - loss: 8.8661e-04 - val_loss: 0.0012 - 747ms/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "108/108 - 1s - loss: 8.4671e-04 - val_loss: 0.0012 - 748ms/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "108/108 - 1s - loss: 8.1090e-04 - val_loss: 0.0012 - 729ms/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "108/108 - 1s - loss: 7.7987e-04 - val_loss: 0.0011 - 742ms/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "108/108 - 1s - loss: 7.4932e-04 - val_loss: 0.0011 - 726ms/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "108/108 - 1s - loss: 7.1234e-04 - val_loss: 0.0011 - 720ms/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "108/108 - 1s - loss: 6.8246e-04 - val_loss: 0.0011 - 725ms/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "108/108 - 1s - loss: 6.5734e-04 - val_loss: 0.0011 - 738ms/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "108/108 - 1s - loss: 6.3677e-04 - val_loss: 0.0011 - 781ms/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "108/108 - 1s - loss: 6.2316e-04 - val_loss: 0.0011 - 709ms/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "108/108 - 1s - loss: 6.1027e-04 - val_loss: 0.0011 - 774ms/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "108/108 - 1s - loss: 5.8780e-04 - val_loss: 0.0012 - 772ms/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "108/108 - 1s - loss: 5.9807e-04 - val_loss: 0.0012 - 733ms/epoch - 7ms/step\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:12:03.750809: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:03.819669: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:03.883660: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:04.763665: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:04.794174: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 2s - loss: 0.0033 - val_loss: 0.0019 - 2s/epoch - 14ms/step\n",
      "Epoch 2/25\n",
      "108/108 - 1s - loss: 0.0046 - val_loss: 0.0071 - 736ms/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "108/108 - 1s - loss: 0.0022 - val_loss: 0.0040 - 735ms/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "108/108 - 1s - loss: 0.0015 - val_loss: 0.0025 - 717ms/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "108/108 - 1s - loss: 0.0012 - val_loss: 0.0018 - 739ms/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0014 - 747ms/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0012 - 740ms/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "108/108 - 1s - loss: 0.0010 - val_loss: 0.0011 - 728ms/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "108/108 - 1s - loss: 9.8693e-04 - val_loss: 9.9958e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "108/108 - 1s - loss: 9.3767e-04 - val_loss: 9.5478e-04 - 718ms/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "108/108 - 1s - loss: 8.8620e-04 - val_loss: 9.2082e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "108/108 - 1s - loss: 8.3755e-04 - val_loss: 8.9316e-04 - 773ms/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "108/108 - 1s - loss: 7.9174e-04 - val_loss: 8.6696e-04 - 745ms/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "108/108 - 1s - loss: 7.5076e-04 - val_loss: 8.4002e-04 - 745ms/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "108/108 - 1s - loss: 7.1361e-04 - val_loss: 8.1700e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "108/108 - 1s - loss: 6.7844e-04 - val_loss: 8.0002e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "108/108 - 1s - loss: 6.4615e-04 - val_loss: 7.9037e-04 - 740ms/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "108/108 - 1s - loss: 6.1925e-04 - val_loss: 7.8783e-04 - 750ms/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "108/108 - 1s - loss: 5.9845e-04 - val_loss: 7.9340e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "108/108 - 1s - loss: 5.8138e-04 - val_loss: 8.0249e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "108/108 - 1s - loss: 5.6675e-04 - val_loss: 8.0989e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "108/108 - 1s - loss: 5.5728e-04 - val_loss: 8.1869e-04 - 764ms/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "108/108 - 1s - loss: 5.6125e-04 - val_loss: 8.5969e-04 - 736ms/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "108/108 - 1s - loss: 5.7352e-04 - val_loss: 8.2013e-04 - 751ms/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "108/108 - 1s - loss: 5.5440e-04 - val_loss: 8.6623e-04 - 720ms/epoch - 7ms/step\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:12:23.352132: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:23.414746: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:23.571292: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 2s - loss: 0.0013 - val_loss: 0.0079 - 2s/epoch - 15ms/step\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:12:24.489340: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:24.521631: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0027 - val_loss: 0.0019 - 735ms/epoch - 7ms/step\n",
      "Epoch 3/25\n",
      "108/108 - 1s - loss: 0.0037 - val_loss: 0.0049 - 751ms/epoch - 7ms/step\n",
      "Epoch 4/25\n",
      "108/108 - 1s - loss: 0.0028 - val_loss: 0.0051 - 750ms/epoch - 7ms/step\n",
      "Epoch 5/25\n",
      "108/108 - 1s - loss: 0.0015 - val_loss: 0.0029 - 725ms/epoch - 7ms/step\n",
      "Epoch 6/25\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0018 - 736ms/epoch - 7ms/step\n",
      "Epoch 7/25\n",
      "108/108 - 1s - loss: 8.3407e-04 - val_loss: 0.0013 - 728ms/epoch - 7ms/step\n",
      "Epoch 8/25\n",
      "108/108 - 1s - loss: 7.7551e-04 - val_loss: 0.0011 - 721ms/epoch - 7ms/step\n",
      "Epoch 9/25\n",
      "108/108 - 1s - loss: 7.6241e-04 - val_loss: 9.7796e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 10/25\n",
      "108/108 - 1s - loss: 7.5918e-04 - val_loss: 9.4061e-04 - 717ms/epoch - 7ms/step\n",
      "Epoch 11/25\n",
      "108/108 - 1s - loss: 7.5345e-04 - val_loss: 9.1933e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 12/25\n",
      "108/108 - 1s - loss: 7.4318e-04 - val_loss: 9.0515e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 13/25\n",
      "108/108 - 1s - loss: 7.2931e-04 - val_loss: 8.9407e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 14/25\n",
      "108/108 - 1s - loss: 7.1382e-04 - val_loss: 8.8353e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 15/25\n",
      "108/108 - 1s - loss: 6.9769e-04 - val_loss: 8.7339e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 16/25\n",
      "108/108 - 1s - loss: 6.8119e-04 - val_loss: 8.6719e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 17/25\n",
      "108/108 - 1s - loss: 6.6558e-04 - val_loss: 8.5469e-04 - 735ms/epoch - 7ms/step\n",
      "Epoch 18/25\n",
      "108/108 - 1s - loss: 6.4959e-04 - val_loss: 8.3648e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 19/25\n",
      "108/108 - 1s - loss: 6.3857e-04 - val_loss: 8.2786e-04 - 793ms/epoch - 7ms/step\n",
      "Epoch 20/25\n",
      "108/108 - 1s - loss: 6.3241e-04 - val_loss: 8.4148e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 21/25\n",
      "108/108 - 1s - loss: 6.4011e-04 - val_loss: 8.7949e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 22/25\n",
      "108/108 - 1s - loss: 6.6567e-04 - val_loss: 8.9864e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 23/25\n",
      "108/108 - 1s - loss: 6.8370e-04 - val_loss: 8.9540e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 24/25\n",
      "108/108 - 1s - loss: 6.6236e-04 - val_loss: 8.9122e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 25/25\n",
      "108/108 - 1s - loss: 6.1369e-04 - val_loss: 9.3641e-04 - 718ms/epoch - 7ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:12:42.625942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:42.696739: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:42.756418: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0025 - val_loss: 0.0042 - 1s/epoch - 13ms/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:12:43.608268: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:12:43.636993: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0037 - val_loss: 0.0021 - 713ms/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "108/108 - 1s - loss: 0.0029 - val_loss: 0.0050 - 716ms/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "108/108 - 1s - loss: 0.0015 - val_loss: 0.0026 - 712ms/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0016 - 714ms/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "108/108 - 1s - loss: 9.3688e-04 - val_loss: 0.0012 - 715ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "108/108 - 1s - loss: 8.9972e-04 - val_loss: 0.0011 - 712ms/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "108/108 - 1s - loss: 8.8154e-04 - val_loss: 9.7948e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "108/108 - 1s - loss: 8.6106e-04 - val_loss: 9.2529e-04 - 716ms/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "108/108 - 1s - loss: 8.3649e-04 - val_loss: 8.8886e-04 - 713ms/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "108/108 - 1s - loss: 8.1013e-04 - val_loss: 8.6563e-04 - 714ms/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "108/108 - 1s - loss: 7.8390e-04 - val_loss: 8.5244e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "108/108 - 1s - loss: 7.5887e-04 - val_loss: 8.4664e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "108/108 - 1s - loss: 7.3548e-04 - val_loss: 8.4588e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "108/108 - 1s - loss: 7.1378e-04 - val_loss: 8.4810e-04 - 716ms/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "108/108 - 1s - loss: 6.9364e-04 - val_loss: 8.5160e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "108/108 - 1s - loss: 6.7484e-04 - val_loss: 8.5521e-04 - 725ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "108/108 - 1s - loss: 6.5718e-04 - val_loss: 8.5853e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "108/108 - 1s - loss: 6.4049e-04 - val_loss: 8.6196e-04 - 717ms/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "108/108 - 1s - loss: 6.2475e-04 - val_loss: 8.6648e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "108/108 - 1s - loss: 6.1008e-04 - val_loss: 8.7314e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "108/108 - 1s - loss: 5.9672e-04 - val_loss: 8.8256e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "108/108 - 1s - loss: 5.8493e-04 - val_loss: 8.9483e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "108/108 - 1s - loss: 5.7487e-04 - val_loss: 9.0964e-04 - 738ms/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "108/108 - 1s - loss: 5.6654e-04 - val_loss: 9.2642e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "108/108 - 1s - loss: 5.5982e-04 - val_loss: 9.4443e-04 - 717ms/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "108/108 - 1s - loss: 5.5452e-04 - val_loss: 9.6296e-04 - 709ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "108/108 - 1s - loss: 5.5026e-04 - val_loss: 9.8105e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "108/108 - 1s - loss: 5.4626e-04 - val_loss: 9.9597e-04 - 712ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "108/108 - 1s - loss: 5.4211e-04 - val_loss: 0.0010 - 713ms/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "108/108 - 1s - loss: 5.4229e-04 - val_loss: 0.0010 - 714ms/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "108/108 - 1s - loss: 5.4384e-04 - val_loss: 0.0010 - 715ms/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "108/108 - 1s - loss: 5.2675e-04 - val_loss: 0.0011 - 717ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "108/108 - 1s - loss: 5.3355e-04 - val_loss: 0.0011 - 717ms/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "108/108 - 1s - loss: 5.4272e-04 - val_loss: 0.0011 - 755ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "108/108 - 1s - loss: 5.2525e-04 - val_loss: 0.0010 - 717ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "108/108 - 1s - loss: 4.9714e-04 - val_loss: 0.0011 - 709ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "108/108 - 1s - loss: 4.9375e-04 - val_loss: 0.0011 - 733ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "108/108 - 1s - loss: 5.0048e-04 - val_loss: 0.0011 - 723ms/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "108/108 - 1s - loss: 5.1367e-04 - val_loss: 0.0012 - 708ms/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "108/108 - 1s - loss: 5.2780e-04 - val_loss: 0.0012 - 711ms/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "108/108 - 1s - loss: 5.0332e-04 - val_loss: 0.0012 - 719ms/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "108/108 - 1s - loss: 4.7054e-04 - val_loss: 0.0012 - 712ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "108/108 - 1s - loss: 4.7804e-04 - val_loss: 0.0012 - 714ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "108/108 - 1s - loss: 4.7627e-04 - val_loss: 0.0012 - 710ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "108/108 - 1s - loss: 4.6663e-04 - val_loss: 0.0012 - 712ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "108/108 - 1s - loss: 4.5399e-04 - val_loss: 0.0012 - 714ms/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "108/108 - 1s - loss: 4.5648e-04 - val_loss: 0.0012 - 713ms/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "108/108 - 1s - loss: 4.5306e-04 - val_loss: 0.0012 - 719ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "108/108 - 1s - loss: 4.4172e-04 - val_loss: 0.0012 - 710ms/epoch - 7ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:13:19.484664: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:19.555888: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:19.620909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0021 - val_loss: 0.0015 - 1s/epoch - 13ms/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:13:20.495390: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:20.523909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0035 - val_loss: 0.0013 - 705ms/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "108/108 - 1s - loss: 0.0030 - val_loss: 0.0036 - 714ms/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "108/108 - 1s - loss: 0.0018 - val_loss: 0.0018 - 714ms/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "108/108 - 1s - loss: 0.0012 - val_loss: 0.0011 - 780ms/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "108/108 - 1s - loss: 0.0010 - val_loss: 8.1661e-04 - 762ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "108/108 - 1s - loss: 9.4158e-04 - val_loss: 7.0662e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "108/108 - 1s - loss: 9.2256e-04 - val_loss: 6.7781e-04 - 718ms/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "108/108 - 1s - loss: 9.0403e-04 - val_loss: 6.7083e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "108/108 - 1s - loss: 8.7542e-04 - val_loss: 6.6816e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "108/108 - 1s - loss: 8.3960e-04 - val_loss: 6.6475e-04 - 715ms/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "108/108 - 1s - loss: 8.0016e-04 - val_loss: 6.5907e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "108/108 - 1s - loss: 7.6142e-04 - val_loss: 6.5375e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "108/108 - 1s - loss: 7.2397e-04 - val_loss: 6.4933e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "108/108 - 1s - loss: 6.9036e-04 - val_loss: 6.4816e-04 - 716ms/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "108/108 - 1s - loss: 6.6024e-04 - val_loss: 6.4913e-04 - 715ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "108/108 - 1s - loss: 6.3568e-04 - val_loss: 6.5341e-04 - 716ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "108/108 - 1s - loss: 6.1540e-04 - val_loss: 6.5987e-04 - 710ms/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "108/108 - 1s - loss: 5.9972e-04 - val_loss: 6.6922e-04 - 712ms/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "108/108 - 1s - loss: 5.8702e-04 - val_loss: 6.7915e-04 - 716ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "108/108 - 1s - loss: 5.7731e-04 - val_loss: 6.9130e-04 - 711ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "108/108 - 1s - loss: 5.6926e-04 - val_loss: 7.0282e-04 - 720ms/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "108/108 - 1s - loss: 5.6295e-04 - val_loss: 7.1564e-04 - 713ms/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "108/108 - 1s - loss: 5.5653e-04 - val_loss: 7.1767e-04 - 714ms/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "108/108 - 1s - loss: 5.5379e-04 - val_loss: 7.3243e-04 - 713ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "108/108 - 1s - loss: 5.6754e-04 - val_loss: 7.4141e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "108/108 - 1s - loss: 5.4524e-04 - val_loss: 7.7129e-04 - 717ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "108/108 - 1s - loss: 5.3817e-04 - val_loss: 7.9665e-04 - 710ms/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "108/108 - 1s - loss: 5.3158e-04 - val_loss: 8.0536e-04 - 845ms/epoch - 8ms/step\n",
      "Epoch 30/50\n",
      "108/108 - 1s - loss: 5.3223e-04 - val_loss: 8.1898e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "108/108 - 1s - loss: 5.2165e-04 - val_loss: 8.3423e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "108/108 - 1s - loss: 5.1476e-04 - val_loss: 8.5211e-04 - 710ms/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "108/108 - 1s - loss: 5.1812e-04 - val_loss: 8.5768e-04 - 713ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "108/108 - 1s - loss: 5.1083e-04 - val_loss: 8.8158e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 35/50\n",
      "108/108 - 1s - loss: 4.9702e-04 - val_loss: 9.0231e-04 - 754ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "108/108 - 1s - loss: 4.8960e-04 - val_loss: 9.3609e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "108/108 - 1s - loss: 4.9011e-04 - val_loss: 9.4113e-04 - 713ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "108/108 - 1s - loss: 4.9224e-04 - val_loss: 9.7102e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "108/108 - 1s - loss: 4.7962e-04 - val_loss: 9.7951e-04 - 715ms/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "108/108 - 1s - loss: 4.6642e-04 - val_loss: 0.0010 - 711ms/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "108/108 - 1s - loss: 4.6029e-04 - val_loss: 0.0010 - 716ms/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "108/108 - 1s - loss: 4.6654e-04 - val_loss: 0.0011 - 714ms/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "108/108 - 1s - loss: 4.7555e-04 - val_loss: 0.0011 - 723ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "108/108 - 1s - loss: 4.6584e-04 - val_loss: 0.0011 - 724ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "108/108 - 1s - loss: 4.4276e-04 - val_loss: 0.0011 - 718ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "108/108 - 1s - loss: 4.4105e-04 - val_loss: 0.0011 - 721ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "108/108 - 1s - loss: 4.3825e-04 - val_loss: 0.0012 - 723ms/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "108/108 - 1s - loss: 4.3908e-04 - val_loss: 0.0012 - 748ms/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "108/108 - 1s - loss: 4.3146e-04 - val_loss: 0.0013 - 750ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "108/108 - 1s - loss: 4.2820e-04 - val_loss: 0.0013 - 717ms/epoch - 7ms/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:13:56.600414: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:56.677970: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:56.742724: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0016 - val_loss: 0.0103 - 1s/epoch - 14ms/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:13:57.635346: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:13:57.663712: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0037 - val_loss: 8.1984e-04 - 742ms/epoch - 7ms/step\n",
      "Epoch 3/50\n",
      "108/108 - 1s - loss: 0.0041 - val_loss: 0.0074 - 731ms/epoch - 7ms/step\n",
      "Epoch 4/50\n",
      "108/108 - 1s - loss: 0.0025 - val_loss: 0.0042 - 727ms/epoch - 7ms/step\n",
      "Epoch 5/50\n",
      "108/108 - 1s - loss: 0.0014 - val_loss: 0.0028 - 737ms/epoch - 7ms/step\n",
      "Epoch 6/50\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0018 - 735ms/epoch - 7ms/step\n",
      "Epoch 7/50\n",
      "108/108 - 1s - loss: 8.5925e-04 - val_loss: 0.0013 - 737ms/epoch - 7ms/step\n",
      "Epoch 8/50\n",
      "108/108 - 1s - loss: 8.0474e-04 - val_loss: 0.0010 - 732ms/epoch - 7ms/step\n",
      "Epoch 9/50\n",
      "108/108 - 1s - loss: 7.9262e-04 - val_loss: 9.2890e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 10/50\n",
      "108/108 - 1s - loss: 7.8647e-04 - val_loss: 8.7072e-04 - 748ms/epoch - 7ms/step\n",
      "Epoch 11/50\n",
      "108/108 - 1s - loss: 7.7485e-04 - val_loss: 8.2508e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 12/50\n",
      "108/108 - 1s - loss: 7.5731e-04 - val_loss: 7.8438e-04 - 738ms/epoch - 7ms/step\n",
      "Epoch 13/50\n",
      "108/108 - 1s - loss: 7.3622e-04 - val_loss: 7.4592e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 14/50\n",
      "108/108 - 1s - loss: 7.1431e-04 - val_loss: 7.0869e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 15/50\n",
      "108/108 - 1s - loss: 6.9314e-04 - val_loss: 6.7521e-04 - 743ms/epoch - 7ms/step\n",
      "Epoch 16/50\n",
      "108/108 - 1s - loss: 6.7327e-04 - val_loss: 6.5333e-04 - 768ms/epoch - 7ms/step\n",
      "Epoch 17/50\n",
      "108/108 - 1s - loss: 6.5606e-04 - val_loss: 6.4700e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 18/50\n",
      "108/108 - 1s - loss: 6.4408e-04 - val_loss: 6.5122e-04 - 767ms/epoch - 7ms/step\n",
      "Epoch 19/50\n",
      "108/108 - 1s - loss: 6.3738e-04 - val_loss: 6.6302e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 20/50\n",
      "108/108 - 1s - loss: 6.3492e-04 - val_loss: 6.7686e-04 - 736ms/epoch - 7ms/step\n",
      "Epoch 21/50\n",
      "108/108 - 1s - loss: 6.3132e-04 - val_loss: 6.6074e-04 - 763ms/epoch - 7ms/step\n",
      "Epoch 22/50\n",
      "108/108 - 1s - loss: 6.2343e-04 - val_loss: 6.6057e-04 - 755ms/epoch - 7ms/step\n",
      "Epoch 23/50\n",
      "108/108 - 1s - loss: 6.4112e-04 - val_loss: 6.3297e-04 - 749ms/epoch - 7ms/step\n",
      "Epoch 24/50\n",
      "108/108 - 1s - loss: 6.0990e-04 - val_loss: 6.3181e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 25/50\n",
      "108/108 - 1s - loss: 5.8695e-04 - val_loss: 6.4495e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 26/50\n",
      "108/108 - 1s - loss: 5.7502e-04 - val_loss: 6.5268e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 27/50\n",
      "108/108 - 1s - loss: 5.7555e-04 - val_loss: 6.6564e-04 - 743ms/epoch - 7ms/step\n",
      "Epoch 28/50\n",
      "108/108 - 1s - loss: 5.8541e-04 - val_loss: 6.8526e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 29/50\n",
      "108/108 - 1s - loss: 5.9128e-04 - val_loss: 6.9984e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 30/50\n",
      "108/108 - 1s - loss: 6.0326e-04 - val_loss: 7.5197e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 31/50\n",
      "108/108 - 1s - loss: 6.2861e-04 - val_loss: 7.5725e-04 - 796ms/epoch - 7ms/step\n",
      "Epoch 32/50\n",
      "108/108 - 1s - loss: 6.6453e-04 - val_loss: 7.7390e-04 - 758ms/epoch - 7ms/step\n",
      "Epoch 33/50\n",
      "108/108 - 1s - loss: 6.3301e-04 - val_loss: 8.5382e-04 - 779ms/epoch - 7ms/step\n",
      "Epoch 34/50\n",
      "108/108 - 1s - loss: 6.1262e-04 - val_loss: 9.0002e-04 - 850ms/epoch - 8ms/step\n",
      "Epoch 35/50\n",
      "108/108 - 1s - loss: 5.7071e-04 - val_loss: 8.7981e-04 - 758ms/epoch - 7ms/step\n",
      "Epoch 36/50\n",
      "108/108 - 1s - loss: 5.0747e-04 - val_loss: 8.8246e-04 - 794ms/epoch - 7ms/step\n",
      "Epoch 37/50\n",
      "108/108 - 1s - loss: 4.8201e-04 - val_loss: 8.6468e-04 - 753ms/epoch - 7ms/step\n",
      "Epoch 38/50\n",
      "108/108 - 1s - loss: 4.7357e-04 - val_loss: 8.3626e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 39/50\n",
      "108/108 - 1s - loss: 4.7190e-04 - val_loss: 8.1482e-04 - 764ms/epoch - 7ms/step\n",
      "Epoch 40/50\n",
      "108/108 - 1s - loss: 4.7483e-04 - val_loss: 7.9379e-04 - 758ms/epoch - 7ms/step\n",
      "Epoch 41/50\n",
      "108/108 - 1s - loss: 4.7780e-04 - val_loss: 7.6396e-04 - 748ms/epoch - 7ms/step\n",
      "Epoch 42/50\n",
      "108/108 - 1s - loss: 4.7512e-04 - val_loss: 7.3358e-04 - 755ms/epoch - 7ms/step\n",
      "Epoch 43/50\n",
      "108/108 - 1s - loss: 4.6598e-04 - val_loss: 7.1410e-04 - 750ms/epoch - 7ms/step\n",
      "Epoch 44/50\n",
      "108/108 - 1s - loss: 4.6864e-04 - val_loss: 7.1391e-04 - 743ms/epoch - 7ms/step\n",
      "Epoch 45/50\n",
      "108/108 - 1s - loss: 4.8727e-04 - val_loss: 7.2595e-04 - 764ms/epoch - 7ms/step\n",
      "Epoch 46/50\n",
      "108/108 - 1s - loss: 5.2036e-04 - val_loss: 7.5915e-04 - 755ms/epoch - 7ms/step\n",
      "Epoch 47/50\n",
      "108/108 - 1s - loss: 5.5914e-04 - val_loss: 8.1766e-04 - 765ms/epoch - 7ms/step\n",
      "Epoch 48/50\n",
      "108/108 - 1s - loss: 6.0350e-04 - val_loss: 8.8308e-04 - 770ms/epoch - 7ms/step\n",
      "Epoch 49/50\n",
      "108/108 - 1s - loss: 6.5698e-04 - val_loss: 0.0010 - 755ms/epoch - 7ms/step\n",
      "Epoch 50/50\n",
      "108/108 - 1s - loss: 7.0501e-04 - val_loss: 0.0012 - 770ms/epoch - 7ms/step\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:14:35.255290: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:14:35.319656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:14:35.401920: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 2s - loss: 0.0024 - val_loss: 8.2690e-04 - 2s/epoch - 14ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:14:36.317964: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:14:36.346831: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0037 - val_loss: 0.0037 - 776ms/epoch - 7ms/step\n",
      "Epoch 3/100\n",
      "108/108 - 1s - loss: 0.0021 - val_loss: 0.0016 - 746ms/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "108/108 - 1s - loss: 0.0014 - val_loss: 8.8196e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 5/100\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 6.4245e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 6/100\n",
      "108/108 - 1s - loss: 9.9997e-04 - val_loss: 5.2290e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 7/100\n",
      "108/108 - 1s - loss: 9.5362e-04 - val_loss: 4.6759e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "108/108 - 1s - loss: 9.2461e-04 - val_loss: 4.3980e-04 - 738ms/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "108/108 - 1s - loss: 8.9977e-04 - val_loss: 4.2337e-04 - 738ms/epoch - 7ms/step\n",
      "Epoch 10/100\n",
      "108/108 - 1s - loss: 8.7399e-04 - val_loss: 4.1427e-04 - 735ms/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "108/108 - 1s - loss: 8.4843e-04 - val_loss: 4.1051e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "108/108 - 1s - loss: 8.2350e-04 - val_loss: 4.1228e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 13/100\n",
      "108/108 - 1s - loss: 8.0141e-04 - val_loss: 4.1864e-04 - 720ms/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "108/108 - 1s - loss: 7.7929e-04 - val_loss: 4.2742e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "108/108 - 1s - loss: 7.5699e-04 - val_loss: 4.3666e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "108/108 - 1s - loss: 7.3114e-04 - val_loss: 4.4546e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "108/108 - 1s - loss: 7.0721e-04 - val_loss: 4.5167e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "108/108 - 1s - loss: 6.8172e-04 - val_loss: 4.5968e-04 - 718ms/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "108/108 - 1s - loss: 6.5967e-04 - val_loss: 4.6972e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "108/108 - 1s - loss: 6.4024e-04 - val_loss: 4.8651e-04 - 907ms/epoch - 8ms/step\n",
      "Epoch 21/100\n",
      "108/108 - 1s - loss: 6.1592e-04 - val_loss: 5.0298e-04 - 778ms/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "108/108 - 1s - loss: 6.0062e-04 - val_loss: 5.1160e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "108/108 - 1s - loss: 5.8449e-04 - val_loss: 5.2086e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "108/108 - 1s - loss: 5.7611e-04 - val_loss: 5.3258e-04 - 739ms/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "108/108 - 1s - loss: 5.7120e-04 - val_loss: 5.4564e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 26/100\n",
      "108/108 - 1s - loss: 5.5616e-04 - val_loss: 5.5758e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "108/108 - 1s - loss: 5.4826e-04 - val_loss: 5.8524e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "108/108 - 1s - loss: 5.3864e-04 - val_loss: 6.0106e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "108/108 - 1s - loss: 5.3711e-04 - val_loss: 6.3216e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "108/108 - 1s - loss: 5.4116e-04 - val_loss: 6.4441e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "108/108 - 1s - loss: 5.3001e-04 - val_loss: 6.6691e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "108/108 - 1s - loss: 5.1972e-04 - val_loss: 6.6824e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 33/100\n",
      "108/108 - 1s - loss: 5.1307e-04 - val_loss: 6.7963e-04 - 741ms/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "108/108 - 1s - loss: 5.0418e-04 - val_loss: 6.7564e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 35/100\n",
      "108/108 - 1s - loss: 5.1534e-04 - val_loss: 7.0530e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "108/108 - 1s - loss: 5.0808e-04 - val_loss: 6.9590e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 37/100\n",
      "108/108 - 1s - loss: 4.9047e-04 - val_loss: 7.4154e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "108/108 - 1s - loss: 4.8822e-04 - val_loss: 7.4595e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "108/108 - 1s - loss: 4.8144e-04 - val_loss: 7.6969e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 40/100\n",
      "108/108 - 1s - loss: 4.8910e-04 - val_loss: 7.6725e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 41/100\n",
      "108/108 - 1s - loss: 4.8841e-04 - val_loss: 8.2140e-04 - 790ms/epoch - 7ms/step\n",
      "Epoch 42/100\n",
      "108/108 - 1s - loss: 4.9508e-04 - val_loss: 8.7113e-04 - 751ms/epoch - 7ms/step\n",
      "Epoch 43/100\n",
      "108/108 - 1s - loss: 4.8781e-04 - val_loss: 8.6898e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 44/100\n",
      "108/108 - 1s - loss: 4.7272e-04 - val_loss: 8.4639e-04 - 757ms/epoch - 7ms/step\n",
      "Epoch 45/100\n",
      "108/108 - 1s - loss: 4.5377e-04 - val_loss: 8.6665e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "108/108 - 1s - loss: 4.3762e-04 - val_loss: 9.1692e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "108/108 - 1s - loss: 4.3403e-04 - val_loss: 9.1070e-04 - 739ms/epoch - 7ms/step\n",
      "Epoch 48/100\n",
      "108/108 - 1s - loss: 4.3142e-04 - val_loss: 9.3816e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 49/100\n",
      "108/108 - 1s - loss: 4.2866e-04 - val_loss: 9.1997e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 50/100\n",
      "108/108 - 1s - loss: 4.2586e-04 - val_loss: 9.6512e-04 - 746ms/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "108/108 - 1s - loss: 4.2471e-04 - val_loss: 9.5356e-04 - 785ms/epoch - 7ms/step\n",
      "Epoch 52/100\n",
      "108/108 - 1s - loss: 4.1963e-04 - val_loss: 9.9631e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 53/100\n",
      "108/108 - 1s - loss: 4.1925e-04 - val_loss: 9.6809e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 54/100\n",
      "108/108 - 1s - loss: 4.2040e-04 - val_loss: 9.6254e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 55/100\n",
      "108/108 - 1s - loss: 4.2404e-04 - val_loss: 9.5967e-04 - 817ms/epoch - 8ms/step\n",
      "Epoch 56/100\n",
      "108/108 - 1s - loss: 4.3740e-04 - val_loss: 0.0010 - 749ms/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "108/108 - 1s - loss: 4.4901e-04 - val_loss: 0.0011 - 729ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "108/108 - 1s - loss: 4.3917e-04 - val_loss: 0.0012 - 723ms/epoch - 7ms/step\n",
      "Epoch 59/100\n",
      "108/108 - 1s - loss: 4.0510e-04 - val_loss: 0.0012 - 733ms/epoch - 7ms/step\n",
      "Epoch 60/100\n",
      "108/108 - 1s - loss: 3.9968e-04 - val_loss: 0.0013 - 723ms/epoch - 7ms/step\n",
      "Epoch 61/100\n",
      "108/108 - 1s - loss: 3.9865e-04 - val_loss: 0.0012 - 726ms/epoch - 7ms/step\n",
      "Epoch 62/100\n",
      "108/108 - 1s - loss: 3.9051e-04 - val_loss: 0.0013 - 725ms/epoch - 7ms/step\n",
      "Epoch 63/100\n",
      "108/108 - 1s - loss: 3.8930e-04 - val_loss: 0.0012 - 729ms/epoch - 7ms/step\n",
      "Epoch 64/100\n",
      "108/108 - 1s - loss: 3.8611e-04 - val_loss: 0.0013 - 722ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "108/108 - 1s - loss: 3.8778e-04 - val_loss: 0.0013 - 723ms/epoch - 7ms/step\n",
      "Epoch 66/100\n",
      "108/108 - 1s - loss: 3.8715e-04 - val_loss: 0.0013 - 751ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "108/108 - 1s - loss: 3.8899e-04 - val_loss: 0.0013 - 719ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "108/108 - 1s - loss: 3.8810e-04 - val_loss: 0.0014 - 738ms/epoch - 7ms/step\n",
      "Epoch 69/100\n",
      "108/108 - 1s - loss: 3.8906e-04 - val_loss: 0.0013 - 714ms/epoch - 7ms/step\n",
      "Epoch 70/100\n",
      "108/108 - 1s - loss: 3.8774e-04 - val_loss: 0.0014 - 712ms/epoch - 7ms/step\n",
      "Epoch 71/100\n",
      "108/108 - 1s - loss: 3.9031e-04 - val_loss: 0.0013 - 717ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "108/108 - 1s - loss: 3.8737e-04 - val_loss: 0.0014 - 722ms/epoch - 7ms/step\n",
      "Epoch 73/100\n",
      "108/108 - 1s - loss: 3.8700e-04 - val_loss: 0.0014 - 722ms/epoch - 7ms/step\n",
      "Epoch 74/100\n",
      "108/108 - 1s - loss: 3.8393e-04 - val_loss: 0.0015 - 730ms/epoch - 7ms/step\n",
      "Epoch 75/100\n",
      "108/108 - 1s - loss: 3.7998e-04 - val_loss: 0.0014 - 725ms/epoch - 7ms/step\n",
      "Epoch 76/100\n",
      "108/108 - 1s - loss: 3.7854e-04 - val_loss: 0.0015 - 732ms/epoch - 7ms/step\n",
      "Epoch 77/100\n",
      "108/108 - 1s - loss: 3.7948e-04 - val_loss: 0.0015 - 723ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "108/108 - 1s - loss: 3.7947e-04 - val_loss: 0.0015 - 725ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "108/108 - 1s - loss: 3.8031e-04 - val_loss: 0.0015 - 715ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "108/108 - 1s - loss: 3.7620e-04 - val_loss: 0.0016 - 722ms/epoch - 7ms/step\n",
      "Epoch 81/100\n",
      "108/108 - 1s - loss: 3.7738e-04 - val_loss: 0.0015 - 725ms/epoch - 7ms/step\n",
      "Epoch 82/100\n",
      "108/108 - 1s - loss: 3.7229e-04 - val_loss: 0.0016 - 722ms/epoch - 7ms/step\n",
      "Epoch 83/100\n",
      "108/108 - 1s - loss: 3.7380e-04 - val_loss: 0.0016 - 723ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "108/108 - 1s - loss: 3.6994e-04 - val_loss: 0.0016 - 726ms/epoch - 7ms/step\n",
      "Epoch 85/100\n",
      "108/108 - 1s - loss: 3.7133e-04 - val_loss: 0.0016 - 720ms/epoch - 7ms/step\n",
      "Epoch 86/100\n",
      "108/108 - 1s - loss: 3.6918e-04 - val_loss: 0.0016 - 720ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "108/108 - 1s - loss: 3.6993e-04 - val_loss: 0.0016 - 726ms/epoch - 7ms/step\n",
      "Epoch 88/100\n",
      "108/108 - 1s - loss: 3.6938e-04 - val_loss: 0.0016 - 721ms/epoch - 7ms/step\n",
      "Epoch 89/100\n",
      "108/108 - 1s - loss: 3.7145e-04 - val_loss: 0.0016 - 748ms/epoch - 7ms/step\n",
      "Epoch 90/100\n",
      "108/108 - 1s - loss: 3.6987e-04 - val_loss: 0.0016 - 722ms/epoch - 7ms/step\n",
      "Epoch 91/100\n",
      "108/108 - 1s - loss: 3.6588e-04 - val_loss: 0.0016 - 719ms/epoch - 7ms/step\n",
      "Epoch 92/100\n",
      "108/108 - 1s - loss: 3.6569e-04 - val_loss: 0.0016 - 723ms/epoch - 7ms/step\n",
      "Epoch 93/100\n",
      "108/108 - 1s - loss: 3.6360e-04 - val_loss: 0.0016 - 727ms/epoch - 7ms/step\n",
      "Epoch 94/100\n",
      "108/108 - 1s - loss: 3.6296e-04 - val_loss: 0.0016 - 723ms/epoch - 7ms/step\n",
      "Epoch 95/100\n",
      "108/108 - 1s - loss: 3.6271e-04 - val_loss: 0.0016 - 737ms/epoch - 7ms/step\n",
      "Epoch 96/100\n",
      "108/108 - 1s - loss: 3.6228e-04 - val_loss: 0.0016 - 725ms/epoch - 7ms/step\n",
      "Epoch 97/100\n",
      "108/108 - 1s - loss: 3.6210e-04 - val_loss: 0.0016 - 729ms/epoch - 7ms/step\n",
      "Epoch 98/100\n",
      "108/108 - 1s - loss: 3.5819e-04 - val_loss: 0.0016 - 735ms/epoch - 7ms/step\n",
      "Epoch 99/100\n",
      "108/108 - 1s - loss: 3.6037e-04 - val_loss: 0.0016 - 726ms/epoch - 7ms/step\n",
      "Epoch 100/100\n",
      "108/108 - 1s - loss: 3.5993e-04 - val_loss: 0.0016 - 720ms/epoch - 7ms/step\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:15:49.566894: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:15:49.636030: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:15:49.698908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0025 - val_loss: 0.0013 - 1s/epoch - 14ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:15:50.599827: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:15:50.628697: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0041 - val_loss: 0.0023 - 749ms/epoch - 7ms/step\n",
      "Epoch 3/100\n",
      "108/108 - 1s - loss: 0.0027 - val_loss: 0.0025 - 732ms/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "108/108 - 1s - loss: 0.0016 - val_loss: 0.0013 - 731ms/epoch - 7ms/step\n",
      "Epoch 5/100\n",
      "108/108 - 1s - loss: 0.0012 - val_loss: 8.1736e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 6/100\n",
      "108/108 - 1s - loss: 0.0010 - val_loss: 6.3020e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 7/100\n",
      "108/108 - 1s - loss: 9.5350e-04 - val_loss: 5.6602e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "108/108 - 1s - loss: 9.2525e-04 - val_loss: 5.4971e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "108/108 - 1s - loss: 8.9901e-04 - val_loss: 5.4564e-04 - 725ms/epoch - 7ms/step\n",
      "Epoch 10/100\n",
      "108/108 - 1s - loss: 8.6736e-04 - val_loss: 5.4488e-04 - 722ms/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "108/108 - 1s - loss: 8.3231e-04 - val_loss: 5.4559e-04 - 719ms/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "108/108 - 1s - loss: 7.9724e-04 - val_loss: 5.4850e-04 - 725ms/epoch - 7ms/step\n",
      "Epoch 13/100\n",
      "108/108 - 1s - loss: 7.6473e-04 - val_loss: 5.5408e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "108/108 - 1s - loss: 7.3499e-04 - val_loss: 5.6265e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "108/108 - 1s - loss: 7.0734e-04 - val_loss: 5.7468e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "108/108 - 1s - loss: 6.8008e-04 - val_loss: 5.8690e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "108/108 - 1s - loss: 6.5484e-04 - val_loss: 5.9577e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "108/108 - 1s - loss: 6.3328e-04 - val_loss: 6.0238e-04 - 835ms/epoch - 8ms/step\n",
      "Epoch 19/100\n",
      "108/108 - 1s - loss: 6.1491e-04 - val_loss: 6.1005e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "108/108 - 1s - loss: 5.9958e-04 - val_loss: 6.1963e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 21/100\n",
      "108/108 - 1s - loss: 5.8682e-04 - val_loss: 6.3075e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "108/108 - 1s - loss: 5.7657e-04 - val_loss: 6.4205e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "108/108 - 1s - loss: 5.6881e-04 - val_loss: 6.5271e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "108/108 - 1s - loss: 5.6317e-04 - val_loss: 6.6265e-04 - 742ms/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "108/108 - 1s - loss: 5.5916e-04 - val_loss: 6.7212e-04 - 822ms/epoch - 8ms/step\n",
      "Epoch 26/100\n",
      "108/108 - 1s - loss: 5.5621e-04 - val_loss: 6.8136e-04 - 761ms/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "108/108 - 1s - loss: 5.5341e-04 - val_loss: 6.9154e-04 - 740ms/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "108/108 - 1s - loss: 5.4975e-04 - val_loss: 7.0764e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "108/108 - 1s - loss: 5.4477e-04 - val_loss: 7.3333e-04 - 739ms/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "108/108 - 1s - loss: 5.3985e-04 - val_loss: 7.4905e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "108/108 - 1s - loss: 5.3449e-04 - val_loss: 7.4201e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "108/108 - 1s - loss: 5.6699e-04 - val_loss: 7.5391e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 33/100\n",
      "108/108 - 1s - loss: 5.7889e-04 - val_loss: 7.8014e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "108/108 - 1s - loss: 5.4423e-04 - val_loss: 8.2151e-04 - 739ms/epoch - 7ms/step\n",
      "Epoch 35/100\n",
      "108/108 - 1s - loss: 5.1748e-04 - val_loss: 8.5812e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "108/108 - 1s - loss: 5.0034e-04 - val_loss: 8.7707e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 37/100\n",
      "108/108 - 1s - loss: 4.8914e-04 - val_loss: 8.8633e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "108/108 - 1s - loss: 4.8786e-04 - val_loss: 8.8256e-04 - 718ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "108/108 - 1s - loss: 4.8381e-04 - val_loss: 8.8392e-04 - 735ms/epoch - 7ms/step\n",
      "Epoch 40/100\n",
      "108/108 - 1s - loss: 4.8150e-04 - val_loss: 8.8155e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 41/100\n",
      "108/108 - 1s - loss: 4.7845e-04 - val_loss: 8.8013e-04 - 740ms/epoch - 7ms/step\n",
      "Epoch 42/100\n",
      "108/108 - 1s - loss: 4.7423e-04 - val_loss: 8.7334e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 43/100\n",
      "108/108 - 1s - loss: 4.6735e-04 - val_loss: 8.6314e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 44/100\n",
      "108/108 - 1s - loss: 4.6070e-04 - val_loss: 8.5699e-04 - 753ms/epoch - 7ms/step\n",
      "Epoch 45/100\n",
      "108/108 - 1s - loss: 4.5726e-04 - val_loss: 8.6589e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "108/108 - 1s - loss: 4.5801e-04 - val_loss: 8.9126e-04 - 758ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "108/108 - 1s - loss: 4.6623e-04 - val_loss: 9.0864e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 48/100\n",
      "108/108 - 1s - loss: 4.7542e-04 - val_loss: 9.1742e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 49/100\n",
      "108/108 - 1s - loss: 4.9477e-04 - val_loss: 9.7254e-04 - 737ms/epoch - 7ms/step\n",
      "Epoch 50/100\n",
      "108/108 - 1s - loss: 5.3515e-04 - val_loss: 0.0010 - 738ms/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "108/108 - 1s - loss: 5.1433e-04 - val_loss: 8.8149e-04 - 772ms/epoch - 7ms/step\n",
      "Epoch 52/100\n",
      "108/108 - 1s - loss: 4.2007e-04 - val_loss: 0.0012 - 731ms/epoch - 7ms/step\n",
      "Epoch 53/100\n",
      "108/108 - 1s - loss: 4.5566e-04 - val_loss: 0.0011 - 729ms/epoch - 7ms/step\n",
      "Epoch 54/100\n",
      "108/108 - 1s - loss: 4.2091e-04 - val_loss: 0.0013 - 729ms/epoch - 7ms/step\n",
      "Epoch 55/100\n",
      "108/108 - 1s - loss: 4.1706e-04 - val_loss: 0.0013 - 725ms/epoch - 7ms/step\n",
      "Epoch 56/100\n",
      "108/108 - 1s - loss: 4.2179e-04 - val_loss: 0.0013 - 727ms/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "108/108 - 1s - loss: 4.1844e-04 - val_loss: 0.0014 - 724ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "108/108 - 1s - loss: 4.3081e-04 - val_loss: 0.0015 - 730ms/epoch - 7ms/step\n",
      "Epoch 59/100\n",
      "108/108 - 1s - loss: 4.2946e-04 - val_loss: 0.0016 - 731ms/epoch - 7ms/step\n",
      "Epoch 60/100\n",
      "108/108 - 1s - loss: 4.3795e-04 - val_loss: 0.0017 - 728ms/epoch - 7ms/step\n",
      "Epoch 61/100\n",
      "108/108 - 1s - loss: 4.3741e-04 - val_loss: 0.0018 - 733ms/epoch - 7ms/step\n",
      "Epoch 62/100\n",
      "108/108 - 1s - loss: 4.3796e-04 - val_loss: 0.0019 - 733ms/epoch - 7ms/step\n",
      "Epoch 63/100\n",
      "108/108 - 1s - loss: 4.3481e-04 - val_loss: 0.0020 - 736ms/epoch - 7ms/step\n",
      "Epoch 64/100\n",
      "108/108 - 1s - loss: 4.3443e-04 - val_loss: 0.0020 - 728ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "108/108 - 1s - loss: 4.3064e-04 - val_loss: 0.0021 - 726ms/epoch - 7ms/step\n",
      "Epoch 66/100\n",
      "108/108 - 1s - loss: 4.3439e-04 - val_loss: 0.0021 - 725ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "108/108 - 1s - loss: 4.3282e-04 - val_loss: 0.0021 - 728ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "108/108 - 1s - loss: 4.4170e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 69/100\n",
      "108/108 - 1s - loss: 4.2818e-04 - val_loss: 0.0022 - 734ms/epoch - 7ms/step\n",
      "Epoch 70/100\n",
      "108/108 - 1s - loss: 4.3433e-04 - val_loss: 0.0021 - 726ms/epoch - 7ms/step\n",
      "Epoch 71/100\n",
      "108/108 - 1s - loss: 4.2747e-04 - val_loss: 0.0022 - 731ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "108/108 - 1s - loss: 4.2800e-04 - val_loss: 0.0022 - 726ms/epoch - 7ms/step\n",
      "Epoch 73/100\n",
      "108/108 - 1s - loss: 4.2440e-04 - val_loss: 0.0022 - 725ms/epoch - 7ms/step\n",
      "Epoch 74/100\n",
      "108/108 - 1s - loss: 4.2375e-04 - val_loss: 0.0022 - 726ms/epoch - 7ms/step\n",
      "Epoch 75/100\n",
      "108/108 - 1s - loss: 4.2143e-04 - val_loss: 0.0022 - 730ms/epoch - 7ms/step\n",
      "Epoch 76/100\n",
      "108/108 - 1s - loss: 4.2284e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 77/100\n",
      "108/108 - 1s - loss: 4.2021e-04 - val_loss: 0.0022 - 725ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "108/108 - 1s - loss: 4.1927e-04 - val_loss: 0.0022 - 727ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "108/108 - 1s - loss: 4.1642e-04 - val_loss: 0.0022 - 723ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "108/108 - 1s - loss: 4.1496e-04 - val_loss: 0.0021 - 726ms/epoch - 7ms/step\n",
      "Epoch 81/100\n",
      "108/108 - 1s - loss: 4.1007e-04 - val_loss: 0.0022 - 726ms/epoch - 7ms/step\n",
      "Epoch 82/100\n",
      "108/108 - 1s - loss: 4.0991e-04 - val_loss: 0.0021 - 727ms/epoch - 7ms/step\n",
      "Epoch 83/100\n",
      "108/108 - 1s - loss: 4.0695e-04 - val_loss: 0.0021 - 724ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "108/108 - 1s - loss: 4.1159e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 85/100\n",
      "108/108 - 1s - loss: 4.0470e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 86/100\n",
      "108/108 - 1s - loss: 4.0359e-04 - val_loss: 0.0021 - 728ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "108/108 - 1s - loss: 3.9821e-04 - val_loss: 0.0021 - 725ms/epoch - 7ms/step\n",
      "Epoch 88/100\n",
      "108/108 - 1s - loss: 3.9869e-04 - val_loss: 0.0021 - 723ms/epoch - 7ms/step\n",
      "Epoch 89/100\n",
      "108/108 - 1s - loss: 3.9880e-04 - val_loss: 0.0020 - 729ms/epoch - 7ms/step\n",
      "Epoch 90/100\n",
      "108/108 - 1s - loss: 3.9992e-04 - val_loss: 0.0021 - 732ms/epoch - 7ms/step\n",
      "Epoch 91/100\n",
      "108/108 - 1s - loss: 3.9539e-04 - val_loss: 0.0021 - 727ms/epoch - 7ms/step\n",
      "Epoch 92/100\n",
      "108/108 - 1s - loss: 3.9641e-04 - val_loss: 0.0021 - 726ms/epoch - 7ms/step\n",
      "Epoch 93/100\n",
      "108/108 - 1s - loss: 3.9327e-04 - val_loss: 0.0021 - 956ms/epoch - 9ms/step\n",
      "Epoch 94/100\n",
      "108/108 - 1s - loss: 3.9428e-04 - val_loss: 0.0020 - 733ms/epoch - 7ms/step\n",
      "Epoch 95/100\n",
      "108/108 - 1s - loss: 3.9108e-04 - val_loss: 0.0021 - 741ms/epoch - 7ms/step\n",
      "Epoch 96/100\n",
      "108/108 - 1s - loss: 3.8914e-04 - val_loss: 0.0020 - 732ms/epoch - 7ms/step\n",
      "Epoch 97/100\n",
      "108/108 - 1s - loss: 3.8830e-04 - val_loss: 0.0021 - 733ms/epoch - 7ms/step\n",
      "Epoch 98/100\n",
      "108/108 - 1s - loss: 3.8776e-04 - val_loss: 0.0020 - 726ms/epoch - 7ms/step\n",
      "Epoch 99/100\n",
      "108/108 - 1s - loss: 3.8217e-04 - val_loss: 0.0020 - 773ms/epoch - 7ms/step\n",
      "Epoch 100/100\n",
      "108/108 - 1s - loss: 3.8474e-04 - val_loss: 0.0020 - 725ms/epoch - 7ms/step\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:17:04.107834: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:17:04.181115: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:17:04.257084: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 2s - loss: 0.0014 - val_loss: 0.0079 - 2s/epoch - 14ms/step\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:17:05.193355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-02-15 21:17:05.222028: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 - 1s - loss: 0.0031 - val_loss: 9.2631e-04 - 741ms/epoch - 7ms/step\n",
      "Epoch 3/100\n",
      "108/108 - 1s - loss: 0.0038 - val_loss: 0.0059 - 719ms/epoch - 7ms/step\n",
      "Epoch 4/100\n",
      "108/108 - 1s - loss: 0.0027 - val_loss: 0.0046 - 722ms/epoch - 7ms/step\n",
      "Epoch 5/100\n",
      "108/108 - 1s - loss: 0.0015 - val_loss: 0.0028 - 765ms/epoch - 7ms/step\n",
      "Epoch 6/100\n",
      "108/108 - 1s - loss: 0.0011 - val_loss: 0.0018 - 739ms/epoch - 7ms/step\n",
      "Epoch 7/100\n",
      "108/108 - 1s - loss: 8.8959e-04 - val_loss: 0.0013 - 728ms/epoch - 7ms/step\n",
      "Epoch 8/100\n",
      "108/108 - 1s - loss: 8.2264e-04 - val_loss: 0.0011 - 734ms/epoch - 7ms/step\n",
      "Epoch 9/100\n",
      "108/108 - 1s - loss: 8.0071e-04 - val_loss: 0.0010 - 734ms/epoch - 7ms/step\n",
      "Epoch 10/100\n",
      "108/108 - 1s - loss: 7.8630e-04 - val_loss: 9.8453e-04 - 746ms/epoch - 7ms/step\n",
      "Epoch 11/100\n",
      "108/108 - 1s - loss: 7.6732e-04 - val_loss: 9.5039e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 12/100\n",
      "108/108 - 1s - loss: 7.4337e-04 - val_loss: 9.1709e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 13/100\n",
      "108/108 - 1s - loss: 7.1751e-04 - val_loss: 8.8574e-04 - 725ms/epoch - 7ms/step\n",
      "Epoch 14/100\n",
      "108/108 - 1s - loss: 6.9277e-04 - val_loss: 8.5943e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 15/100\n",
      "108/108 - 1s - loss: 6.7109e-04 - val_loss: 8.4422e-04 - 734ms/epoch - 7ms/step\n",
      "Epoch 16/100\n",
      "108/108 - 1s - loss: 6.5329e-04 - val_loss: 8.3749e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "108/108 - 1s - loss: 6.3902e-04 - val_loss: 8.3246e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 18/100\n",
      "108/108 - 1s - loss: 6.2663e-04 - val_loss: 8.2614e-04 - 733ms/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "108/108 - 1s - loss: 6.2799e-04 - val_loss: 8.3194e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 20/100\n",
      "108/108 - 1s - loss: 6.1981e-04 - val_loss: 8.4514e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 21/100\n",
      "108/108 - 1s - loss: 6.0916e-04 - val_loss: 8.6830e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 22/100\n",
      "108/108 - 1s - loss: 6.1986e-04 - val_loss: 8.3361e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 23/100\n",
      "108/108 - 1s - loss: 5.9765e-04 - val_loss: 8.0929e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 24/100\n",
      "108/108 - 1s - loss: 5.8574e-04 - val_loss: 8.1790e-04 - 724ms/epoch - 7ms/step\n",
      "Epoch 25/100\n",
      "108/108 - 1s - loss: 5.8732e-04 - val_loss: 8.1303e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 26/100\n",
      "108/108 - 1s - loss: 5.9203e-04 - val_loss: 8.6162e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 27/100\n",
      "108/108 - 1s - loss: 6.1843e-04 - val_loss: 9.1885e-04 - 772ms/epoch - 7ms/step\n",
      "Epoch 28/100\n",
      "108/108 - 1s - loss: 6.2271e-04 - val_loss: 8.9055e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 29/100\n",
      "108/108 - 1s - loss: 5.8608e-04 - val_loss: 9.0817e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 30/100\n",
      "108/108 - 1s - loss: 5.4730e-04 - val_loss: 9.3546e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "108/108 - 1s - loss: 5.3657e-04 - val_loss: 9.5225e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 32/100\n",
      "108/108 - 1s - loss: 5.3011e-04 - val_loss: 9.5685e-04 - 725ms/epoch - 7ms/step\n",
      "Epoch 33/100\n",
      "108/108 - 1s - loss: 5.2489e-04 - val_loss: 9.6196e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "108/108 - 1s - loss: 5.2794e-04 - val_loss: 9.6602e-04 - 730ms/epoch - 7ms/step\n",
      "Epoch 35/100\n",
      "108/108 - 1s - loss: 5.3257e-04 - val_loss: 9.8512e-04 - 726ms/epoch - 7ms/step\n",
      "Epoch 36/100\n",
      "108/108 - 1s - loss: 5.2384e-04 - val_loss: 9.8585e-04 - 727ms/epoch - 7ms/step\n",
      "Epoch 37/100\n",
      "108/108 - 1s - loss: 5.1688e-04 - val_loss: 9.9285e-04 - 732ms/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "108/108 - 1s - loss: 5.1388e-04 - val_loss: 9.7667e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "108/108 - 1s - loss: 4.9896e-04 - val_loss: 9.8085e-04 - 721ms/epoch - 7ms/step\n",
      "Epoch 40/100\n",
      "108/108 - 1s - loss: 4.8376e-04 - val_loss: 9.8335e-04 - 723ms/epoch - 7ms/step\n",
      "Epoch 41/100\n",
      "108/108 - 1s - loss: 4.7392e-04 - val_loss: 9.8190e-04 - 729ms/epoch - 7ms/step\n",
      "Epoch 42/100\n",
      "108/108 - 1s - loss: 4.7280e-04 - val_loss: 9.8125e-04 - 731ms/epoch - 7ms/step\n",
      "Epoch 43/100\n",
      "108/108 - 1s - loss: 4.8283e-04 - val_loss: 9.9512e-04 - 728ms/epoch - 7ms/step\n",
      "Epoch 44/100\n",
      "108/108 - 1s - loss: 4.8644e-04 - val_loss: 0.0010 - 728ms/epoch - 7ms/step\n",
      "Epoch 45/100\n",
      "108/108 - 1s - loss: 4.7286e-04 - val_loss: 9.9391e-04 - 779ms/epoch - 7ms/step\n",
      "Epoch 46/100\n",
      "108/108 - 1s - loss: 4.6819e-04 - val_loss: 0.0010 - 732ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "108/108 - 1s - loss: 4.7878e-04 - val_loss: 0.0010 - 734ms/epoch - 7ms/step\n",
      "Epoch 48/100\n",
      "108/108 - 1s - loss: 4.7697e-04 - val_loss: 0.0011 - 739ms/epoch - 7ms/step\n",
      "Epoch 49/100\n",
      "108/108 - 1s - loss: 4.7917e-04 - val_loss: 0.0011 - 726ms/epoch - 7ms/step\n",
      "Epoch 50/100\n",
      "108/108 - 1s - loss: 4.7801e-04 - val_loss: 0.0012 - 734ms/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "108/108 - 1s - loss: 4.7156e-04 - val_loss: 0.0012 - 731ms/epoch - 7ms/step\n",
      "Epoch 52/100\n",
      "108/108 - 1s - loss: 4.5941e-04 - val_loss: 0.0012 - 751ms/epoch - 7ms/step\n",
      "Epoch 53/100\n",
      "108/108 - 1s - loss: 4.2844e-04 - val_loss: 0.0013 - 724ms/epoch - 7ms/step\n",
      "Epoch 54/100\n",
      "108/108 - 1s - loss: 4.3690e-04 - val_loss: 0.0013 - 727ms/epoch - 7ms/step\n",
      "Epoch 55/100\n",
      "108/108 - 1s - loss: 4.3999e-04 - val_loss: 0.0013 - 743ms/epoch - 7ms/step\n",
      "Epoch 56/100\n",
      "108/108 - 1s - loss: 4.4286e-04 - val_loss: 0.0014 - 736ms/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "108/108 - 1s - loss: 4.4188e-04 - val_loss: 0.0015 - 727ms/epoch - 7ms/step\n",
      "Epoch 58/100\n",
      "108/108 - 1s - loss: 4.5126e-04 - val_loss: 0.0016 - 734ms/epoch - 7ms/step\n",
      "Epoch 59/100\n",
      "108/108 - 1s - loss: 4.4623e-04 - val_loss: 0.0017 - 731ms/epoch - 7ms/step\n",
      "Epoch 60/100\n",
      "108/108 - 1s - loss: 4.6400e-04 - val_loss: 0.0017 - 726ms/epoch - 7ms/step\n",
      "Epoch 61/100\n",
      "108/108 - 1s - loss: 4.4129e-04 - val_loss: 0.0019 - 730ms/epoch - 7ms/step\n",
      "Epoch 62/100\n",
      "108/108 - 1s - loss: 4.4757e-04 - val_loss: 0.0019 - 727ms/epoch - 7ms/step\n",
      "Epoch 63/100\n",
      "108/108 - 1s - loss: 4.3923e-04 - val_loss: 0.0020 - 730ms/epoch - 7ms/step\n",
      "Epoch 64/100\n",
      "108/108 - 1s - loss: 4.3652e-04 - val_loss: 0.0020 - 731ms/epoch - 7ms/step\n",
      "Epoch 65/100\n",
      "108/108 - 1s - loss: 4.3457e-04 - val_loss: 0.0021 - 730ms/epoch - 7ms/step\n",
      "Epoch 66/100\n",
      "108/108 - 1s - loss: 4.3395e-04 - val_loss: 0.0020 - 726ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "108/108 - 1s - loss: 4.2810e-04 - val_loss: 0.0021 - 724ms/epoch - 7ms/step\n",
      "Epoch 68/100\n",
      "108/108 - 1s - loss: 4.2546e-04 - val_loss: 0.0021 - 752ms/epoch - 7ms/step\n",
      "Epoch 69/100\n",
      "108/108 - 1s - loss: 4.2733e-04 - val_loss: 0.0021 - 734ms/epoch - 7ms/step\n",
      "Epoch 70/100\n",
      "108/108 - 1s - loss: 4.2557e-04 - val_loss: 0.0021 - 728ms/epoch - 7ms/step\n",
      "Epoch 71/100\n",
      "108/108 - 1s - loss: 4.2716e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 72/100\n",
      "108/108 - 1s - loss: 4.2753e-04 - val_loss: 0.0022 - 732ms/epoch - 7ms/step\n",
      "Epoch 73/100\n",
      "108/108 - 1s - loss: 4.2830e-04 - val_loss: 0.0022 - 727ms/epoch - 7ms/step\n",
      "Epoch 74/100\n",
      "108/108 - 1s - loss: 4.2702e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 75/100\n",
      "108/108 - 1s - loss: 4.1921e-04 - val_loss: 0.0022 - 725ms/epoch - 7ms/step\n",
      "Epoch 76/100\n",
      "108/108 - 1s - loss: 4.2118e-04 - val_loss: 0.0021 - 732ms/epoch - 7ms/step\n",
      "Epoch 77/100\n",
      "108/108 - 1s - loss: 4.1943e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 78/100\n",
      "108/108 - 1s - loss: 4.1987e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "108/108 - 1s - loss: 4.1801e-04 - val_loss: 0.0022 - 731ms/epoch - 7ms/step\n",
      "Epoch 80/100\n",
      "108/108 - 1s - loss: 4.2030e-04 - val_loss: 0.0021 - 727ms/epoch - 7ms/step\n",
      "Epoch 81/100\n",
      "108/108 - 1s - loss: 4.1188e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 82/100\n",
      "108/108 - 1s - loss: 4.0864e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 83/100\n",
      "108/108 - 1s - loss: 4.0999e-04 - val_loss: 0.0021 - 738ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "108/108 - 1s - loss: 4.0906e-04 - val_loss: 0.0021 - 733ms/epoch - 7ms/step\n",
      "Epoch 85/100\n",
      "108/108 - 1s - loss: 4.1201e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 86/100\n",
      "108/108 - 1s - loss: 4.0847e-04 - val_loss: 0.0021 - 733ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "108/108 - 1s - loss: 4.0617e-04 - val_loss: 0.0021 - 759ms/epoch - 7ms/step\n",
      "Epoch 88/100\n",
      "108/108 - 1s - loss: 4.0397e-04 - val_loss: 0.0022 - 729ms/epoch - 7ms/step\n",
      "Epoch 89/100\n",
      "108/108 - 1s - loss: 4.0404e-04 - val_loss: 0.0022 - 730ms/epoch - 7ms/step\n",
      "Epoch 90/100\n",
      "108/108 - 1s - loss: 4.0372e-04 - val_loss: 0.0022 - 730ms/epoch - 7ms/step\n",
      "Epoch 91/100\n",
      "108/108 - 1s - loss: 4.0388e-04 - val_loss: 0.0021 - 739ms/epoch - 7ms/step\n",
      "Epoch 92/100\n",
      "108/108 - 1s - loss: 3.9553e-04 - val_loss: 0.0021 - 728ms/epoch - 7ms/step\n",
      "Epoch 93/100\n",
      "108/108 - 1s - loss: 3.9651e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 94/100\n",
      "108/108 - 1s - loss: 3.9552e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 95/100\n",
      "108/108 - 1s - loss: 3.9702e-04 - val_loss: 0.0021 - 731ms/epoch - 7ms/step\n",
      "Epoch 96/100\n",
      "108/108 - 1s - loss: 3.9221e-04 - val_loss: 0.0021 - 730ms/epoch - 7ms/step\n",
      "Epoch 97/100\n",
      "108/108 - 1s - loss: 3.9023e-04 - val_loss: 0.0020 - 734ms/epoch - 7ms/step\n",
      "Epoch 98/100\n",
      "108/108 - 1s - loss: 3.8773e-04 - val_loss: 0.0021 - 730ms/epoch - 7ms/step\n",
      "Epoch 99/100\n",
      "108/108 - 1s - loss: 3.8965e-04 - val_loss: 0.0021 - 729ms/epoch - 7ms/step\n",
      "Epoch 100/100\n",
      "108/108 - 1s - loss: 3.8643e-04 - val_loss: 0.0020 - 726ms/epoch - 7ms/step\n"
     ]
    }
   ],
   "source": [
    "lstms1 = []\n",
    "models1 = []\n",
    "for batch, epoch, neuron in hyperparam1:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neuron, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(loss='mse',optimizer='adam')\n",
    "    lstm = model.fit(train_X, train_y, epochs=epoch, batch_size=batch, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    lstms1.append(lstm)\n",
    "    models1.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327f83dc-5358-43d9-88df-f4b70afa1dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1cf58c29-2143-4f38-922d-4cf5b689458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.03457522]\n",
      "MAE\n",
      "[0.03071824]\n",
      "MAPE\n",
      "[10.283894]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.02943179]\n",
      "MAE\n",
      "[0.02579465]\n",
      "MAPE\n",
      "[8.810367]\n",
      "6/6 [==============================] - 0s 8ms/step\n",
      "RMSE\n",
      "[0.03060083]\n",
      "MAE\n",
      "[0.02708519]\n",
      "MAPE\n",
      "[9.187714]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.03522087]\n",
      "MAE\n",
      "[0.03176555]\n",
      "MAPE\n",
      "[10.602008]\n",
      "6/6 [==============================] - 0s 6ms/step\n",
      "RMSE\n",
      "[0.03609582]\n",
      "MAE\n",
      "[0.03248367]\n",
      "MAPE\n",
      "[10.7929325]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.03399112]\n",
      "MAE\n",
      "[0.03023492]\n",
      "MAPE\n",
      "[10.155189]\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "RMSE\n",
      "[0.03999243]\n",
      "MAE\n",
      "[0.03614402]\n",
      "MAPE\n",
      "[11.83733]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.0445492]\n",
      "MAE\n",
      "[0.04021761]\n",
      "MAPE\n",
      "[12.965089]\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "RMSE\n",
      "[0.04518748]\n",
      "MAE\n",
      "[0.04101871]\n",
      "MAPE\n",
      "[13.208814]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for x in models1:\n",
    "    # make a prediction\n",
    "    # test_x2 = test_X\n",
    "    # yhat = x.predict(test_x2)\n",
    "    # test_x2 = test_x2.reshape((test_x2.shape[0], test_x2.shape[2]))\n",
    "    # # invert scaling for forecast\n",
    "    # inv_yhat = np.concatenate((test_x2[:, :],yhat), axis=1)\n",
    "    # inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    # # inv_yhat = inv_yhat[:,0]\n",
    "    # # invert scaling for actual\n",
    "    # test_y = test_y.reshape((len(test_y), 1))\n",
    "    # inv_y = np.concatenate((test_x2[:, :], test_y), axis=1)\n",
    "    # inv_y = scaler.inverse_transform(inv_y)\n",
    "    # # inv_y = inv_y[:,0]\n",
    "    # yhat_df = pd.DataFrame(inv_yhat, columns = ['Open','High','Low', 'Volume', 'Close', 'Target'])\n",
    "    # y_df = pd.DataFrame(inv_y, columns = ['Open','High','Low', 'Volume', 'Close', 'Target'])\n",
    "    # print(hyperparam1[i])\n",
    "    # print(\"Epoch: \"+ str(lstms1[i].params['epochs']))\n",
    "    # print(\"Neurons: \"+str(x.layers[0].units))\n",
    "    # # print(str(lstms[i].params))\n",
    "    # i = i+1\n",
    "    test_x2 = test_X\n",
    "    test_x3 = x.predict(test_x2)\n",
    "    # print(test_x3.shape)\n",
    "    # print(test_y.shape)\n",
    "    print('RMSE')\n",
    "    print(RMSE(test_x3,test_y))\n",
    "    print('MAE')\n",
    "    print(MAE(test_x3,test_y))\n",
    "    print('MAPE')\n",
    "    print(MAPE(test_x3,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b20106f2-8e22-49d3-8ca1-3f22884a5538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebe15d8d-6bc9-4220-a7db-48e4d4c7cd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 11ms/step\n",
      "(16, 25, 50)\n",
      "Epoch: 25\n",
      "Neurons: 50\n",
      "RMSE\n",
      "2224.459219284565\n",
      "MAE\n",
      "1976.3141181765832\n",
      "MAPE\n",
      "9.946139129078627\n",
      "6/6 [==============================] - 0s 5ms/step\n",
      "(16, 25, 60)\n",
      "Epoch: 25\n",
      "Neurons: 60\n",
      "RMSE\n",
      "1893.547573661105\n",
      "MAE\n",
      "1659.5458418080011\n",
      "MAPE\n",
      "8.358792246114923\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "(16, 25, 100)\n",
      "Epoch: 25\n",
      "Neurons: 100\n",
      "RMSE\n",
      "1968.7598269088612\n",
      "MAE\n",
      "1742.5749648187661\n",
      "MAPE\n",
      "8.748261509970764\n",
      "6/6 [==============================] - 0s 6ms/step\n",
      "(16, 50, 50)\n",
      "Epoch: 50\n",
      "Neurons: 50\n",
      "RMSE\n",
      "2265.9985149453464\n",
      "MAE\n",
      "2043.6941108071774\n",
      "MAPE\n",
      "10.25769405989276\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "(16, 50, 60)\n",
      "Epoch: 50\n",
      "Neurons: 60\n",
      "RMSE\n",
      "2322.290182812373\n",
      "MAE\n",
      "2089.8961041423513\n",
      "MAPE\n",
      "10.475766789899925\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "(16, 50, 100)\n",
      "Epoch: 50\n",
      "Neurons: 100\n",
      "RMSE\n",
      "2186.8800380868565\n",
      "MAE\n",
      "1945.2183353973137\n",
      "MAPE\n",
      "9.793090187473009\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "(16, 100, 50)\n",
      "Epoch: 100\n",
      "Neurons: 50\n",
      "RMSE\n",
      "2572.9855496537484\n",
      "MAE\n",
      "2325.390982162459\n",
      "MAPE\n",
      "11.650254765575772\n",
      "6/6 [==============================] - 0s 4ms/step\n",
      "(16, 100, 60)\n",
      "Epoch: 100\n",
      "Neurons: 60\n",
      "RMSE\n",
      "2866.1534970811426\n",
      "MAE\n",
      "2587.4728380606716\n",
      "MAPE\n",
      "12.976173084101783\n",
      "6/6 [==============================] - 0s 7ms/step\n",
      "(16, 100, 100)\n",
      "Epoch: 100\n",
      "Neurons: 100\n",
      "RMSE\n",
      "2907.218114868605\n",
      "MAE\n",
      "2639.013293846973\n",
      "MAPE\n",
      "13.242370892844967\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for x in models1:\n",
    "    # make a prediction\n",
    "    test_x2 = test_X\n",
    "    yhat = x.predict(test_x2)\n",
    "    # test_x2 = test_x2.reshape((test_x2.shape[0], test_x2.shape[2]))\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((np.zeros((len(yhat), 4)),yhat), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    # inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((np.zeros((len(yhat), 4)), test_y), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    # inv_y = inv_y[:,0]\n",
    "    yhat_df = pd.DataFrame(inv_yhat, columns = ['Open','High','Low', 'Volume', 'Close'])\n",
    "    y_df = pd.DataFrame(inv_y, columns = ['Open','High','Low', 'Volume', 'Close'])\n",
    "    print(hyperparam1[i])\n",
    "    print(\"Epoch: \"+ str(lstms1[i].params['epochs']))\n",
    "    print(\"Neurons: \"+str(x.layers[0].units))\n",
    "    # print(str(lstms[i].params))\n",
    "    i = i+1\n",
    "    # test_x2 = test_X\n",
    "    # test_x3 = x.predict(test_x2)\n",
    "    # # print(test_x3.shape)\n",
    "    # # print(test_y.shape)\n",
    "    # print(yhat_df)\n",
    "    print('RMSE')\n",
    "    print(RMSE(y_df['Close'],yhat_df['Close']))\n",
    "    print('MAE')\n",
    "    print(MAE(y_df['Close'],yhat_df['Close']))\n",
    "    print('MAPE')\n",
    "    print(MAPE(y_df['Close'],yhat_df['Close']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b915d08f-2ba6-410c-92d9-624ff5b63395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>31801.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>29805.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>30452.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>29700.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>29864.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>16522.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>16458.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>16428.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>16212.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>16442.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open    High     Low     Volume     Close\n",
       "0    3189.02  3276.5  2817.0  967753.32  31801.04\n",
       "1    3189.02  3276.5  2817.0  967753.32  29805.83\n",
       "2    3189.02  3276.5  2817.0  967753.32  30452.62\n",
       "3    3189.02  3276.5  2817.0  967753.32  29700.21\n",
       "4    3189.02  3276.5  2817.0  967753.32  29864.04\n",
       "..       ...     ...     ...        ...       ...\n",
       "178  3189.02  3276.5  2817.0  967753.32  16522.14\n",
       "179  3189.02  3276.5  2817.0  967753.32  16458.57\n",
       "180  3189.02  3276.5  2817.0  967753.32  16428.78\n",
       "181  3189.02  3276.5  2817.0  967753.32  16212.91\n",
       "182  3189.02  3276.5  2817.0  967753.32  16442.53\n",
       "\n",
       "[183 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "238df289-9572-4630-9b3b-755e313a919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>Volume USDT</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>8562.04</td>\n",
       "      <td>8978.26</td>\n",
       "      <td>8528.78</td>\n",
       "      <td>7.601955e+08</td>\n",
       "      <td>8810.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>8810.99</td>\n",
       "      <td>9398.00</td>\n",
       "      <td>8792.99</td>\n",
       "      <td>8.397257e+08</td>\n",
       "      <td>9309.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>9309.35</td>\n",
       "      <td>9939.00</td>\n",
       "      <td>9256.76</td>\n",
       "      <td>1.247457e+09</td>\n",
       "      <td>9791.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>9791.97</td>\n",
       "      <td>9845.62</td>\n",
       "      <td>9150.00</td>\n",
       "      <td>1.099721e+09</td>\n",
       "      <td>9316.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>9315.96</td>\n",
       "      <td>9588.00</td>\n",
       "      <td>9220.00</td>\n",
       "      <td>5.605470e+08</td>\n",
       "      <td>9381.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>16521.35</td>\n",
       "      <td>16701.99</td>\n",
       "      <td>16385.00</td>\n",
       "      <td>3.011990e+09</td>\n",
       "      <td>16458.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>16457.61</td>\n",
       "      <td>16600.00</td>\n",
       "      <td>16401.00</td>\n",
       "      <td>2.679049e+09</td>\n",
       "      <td>16428.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>16428.77</td>\n",
       "      <td>16487.04</td>\n",
       "      <td>15995.27</td>\n",
       "      <td>4.096399e+09</td>\n",
       "      <td>16212.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>16212.18</td>\n",
       "      <td>16548.71</td>\n",
       "      <td>16100.00</td>\n",
       "      <td>4.072651e+09</td>\n",
       "      <td>16442.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>16442.91</td>\n",
       "      <td>16466.66</td>\n",
       "      <td>16428.30</td>\n",
       "      <td>3.822050e+07</td>\n",
       "      <td>16443.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>933 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open      high       low   Volume USDT     close\n",
       "1000   8562.04   8978.26   8528.78  7.601955e+08   8810.79\n",
       "1001   8810.99   9398.00   8792.99  8.397257e+08   9309.37\n",
       "1002   9309.35   9939.00   9256.76  1.247457e+09   9791.98\n",
       "1003   9791.97   9845.62   9150.00  1.099721e+09   9316.42\n",
       "1004   9315.96   9588.00   9220.00  5.605470e+08   9381.27\n",
       "...        ...       ...       ...           ...       ...\n",
       "1928  16521.35  16701.99  16385.00  3.011990e+09  16458.57\n",
       "1929  16457.61  16600.00  16401.00  2.679049e+09  16428.78\n",
       "1930  16428.77  16487.04  15995.27  4.096399e+09  16212.91\n",
       "1931  16212.18  16548.71  16100.00  4.072651e+09  16442.53\n",
       "1932  16442.91  16466.66  16428.30  3.822050e+07  16443.60\n",
       "\n",
       "[933 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19068bd2-cf9e-4027-8d97-df95ca7ffe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>32264.634795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>32571.742567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>30858.672904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>31136.158975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>30144.212567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>19472.906784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>19325.448269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>19298.493652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>19174.947821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>3189.02</td>\n",
       "      <td>3276.5</td>\n",
       "      <td>2817.0</td>\n",
       "      <td>967753.32</td>\n",
       "      <td>19215.144867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open    High     Low     Volume         Close\n",
       "0    3189.02  3276.5  2817.0  967753.32  32264.634795\n",
       "1    3189.02  3276.5  2817.0  967753.32  32571.742567\n",
       "2    3189.02  3276.5  2817.0  967753.32  30858.672904\n",
       "3    3189.02  3276.5  2817.0  967753.32  31136.158975\n",
       "4    3189.02  3276.5  2817.0  967753.32  30144.212567\n",
       "..       ...     ...     ...        ...           ...\n",
       "178  3189.02  3276.5  2817.0  967753.32  19472.906784\n",
       "179  3189.02  3276.5  2817.0  967753.32  19325.448269\n",
       "180  3189.02  3276.5  2817.0  967753.32  19298.493652\n",
       "181  3189.02  3276.5  2817.0  967753.32  19174.947821\n",
       "182  3189.02  3276.5  2817.0  967753.32  19215.144867\n",
       "\n",
       "[183 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b3194-7300-4cab-bf14-cade7d1dc5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
